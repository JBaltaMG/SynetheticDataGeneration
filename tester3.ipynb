{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4794b1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import utils.utils as utils\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re, hashlib, json, dotenv\n",
    "from openai import OpenAI\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from utils.utils_llm import create_mapping_from_metadata\n",
    "from typing import Dict, Any, List, Optional\n",
    "import time\n",
    "import random\n",
    "\n",
    "def _overlap_tokens(s: str) -> set:\n",
    "    return set(re.findall(r\"[a-z0-9]+\", str(s).lower()))\n",
    "\n",
    "def _shortlist_accounts(df_accounts, allowed, item_name, k=80):\n",
    "    toks = _overlap_tokens(item_name)\n",
    "    sub = df_accounts[df_accounts[\"GLLevel02\"] == allowed][[\"AccountKey\",\"name\",\"GLLevel02\"]].dropna().copy()\n",
    "    sub[\"AccountKey\"] = sub[\"AccountKey\"].astype(str)\n",
    "    sub[\"__score\"] = sub[\"name\"].astype(str).apply(lambda s: len(toks & _overlap_tokens(s)))\n",
    "    sub = sub.sort_values(\"__score\", ascending=False).head(k).drop(columns=\"__score\")\n",
    "\n",
    "    records = []\n",
    "    for i, r in sub.reset_index(drop=True).iterrows():\n",
    "        records.append({\n",
    "            \"idx\": i,\n",
    "            \"AccountKey\": r[\"AccountKey\"],\n",
    "            \"name\": str(r[\"name\"]),             # ← use the column, not r.name\n",
    "            \"GLLevel02\": r[\"GLLevel02\"],\n",
    "        })\n",
    "    return records\n",
    "\n",
    "def _shortlist_partners(df_partners, item_name, account_name, k=200):\n",
    "    if df_partners is None or \"name\" not in df_partners.columns or df_partners.empty:\n",
    "        return []\n",
    "    toks = _overlap_tokens(item_name) | _overlap_tokens(account_name)\n",
    "    sub = df_partners.copy()\n",
    "    sub[\"__score\"] = sub[\"name\"].astype(str).apply(lambda s: len(toks & _overlap_tokens(s)))\n",
    "    sub = sub.sort_values(\"__score\", ascending=False).head(k).drop(columns=\"__score\")\n",
    "    return [{\"idx\":i,\"name\":n} for i,n in enumerate(sub[\"name\"].astype(str).tolist())]\n",
    "\n",
    "def _det_pick_idx(n: int, key: str) -> int:\n",
    "    h = hashlib.sha1(key.encode(\"utf-8\")).hexdigest()\n",
    "    return int(h, 16) % max(1, n)\n",
    "\n",
    "def _pick_idx_with_gpt(client, system_schema_text, payload, n, fallback_key, model=\"gpt-4.1\", retries=2):\n",
    "    backoff = 0.6\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                response_format={\"type\":\"json_object\"},\n",
    "                messages=[\n",
    "                    {\"role\":\"system\",\"content\":system_schema_text},\n",
    "                    {\"role\":\"user\",\"content\":json.dumps(payload)},\n",
    "                ],\n",
    "                temperature=1,\n",
    "            )\n",
    "            data = json.loads(resp.choices[0].message.content)\n",
    "            idx = next(iter(data.values()))\n",
    "            if isinstance(idx, int) and 0 <= idx < n:\n",
    "                return idx\n",
    "        except Exception:\n",
    "            pass\n",
    "        if attempt < retries:\n",
    "            time.sleep(backoff + random.random() * 0.3)\n",
    "            backoff *= 1.6\n",
    "    # deterministic fallback\n",
    "    return _det_pick_idx(n, fallback_key)\n",
    "\n",
    "\n",
    "def _parse_refs(val: object) -> set[str]:\n",
    "    \"\"\"Accept NaN or string; split on ; | , and collapse whitespace.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return set()\n",
    "    parts = re.split(r\"[;|,]+\", str(val))\n",
    "    return {p.strip() for p in parts if p and p.strip()}\n",
    "\n",
    "\n",
    "def create_mapping_parallel_simple(\n",
    "    df_items: pd.DataFrame,\n",
    "    df_accounts: pd.DataFrame,\n",
    "    df_customers: Optional[pd.DataFrame]=None,\n",
    "    df_vendors: Optional[pd.DataFrame]=None,\n",
    "    *,\n",
    "    df_bu_companies: Optional[pd.DataFrame]=None,   # expects bu_key, sells_to, buys_from\n",
    "    name_column: str=\"product_name\",\n",
    "    model: str=\"gpt-4.1\",\n",
    "    max_workers: int=12\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    # --- validation (as before) ---\n",
    "    for req in [[\"name\"]]:\n",
    "        miss = [c for c in req if c not in df_items.columns]\n",
    "        if miss: raise ValueError(f\"df_items missing {miss}\")\n",
    "    miss = [c for c in [\"AccountKey\",\"name\",\"GLLevel02\"] if c not in df_accounts.columns]\n",
    "    if miss: raise ValueError(f\"df_accounts missing {miss}\")\n",
    "\n",
    "    ACC_ALLOWED = {\n",
    "        \"product_name\": \"Net Sales\",\n",
    "        \"service_name\": \"Operating Expenses\",\n",
    "        \"procurement_name\": \"Cost Of Sales\",\n",
    "    }[name_column]\n",
    "\n",
    "    dotenv.load_dotenv()\n",
    "    api_key = dotenv.get_key(\".env\",\"api_key\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    # --- NEW: build BU relation maps ---\n",
    "    bu_key_set: set[str] = set()\n",
    "    sells_map: dict[str, set[str]] = {}\n",
    "    buys_map:  dict[str, set[str]] = {}\n",
    "    bu_names_df = None\n",
    "\n",
    "    if df_bu_companies is not None and not df_bu_companies.empty:\n",
    "        if \"bu_key\" not in df_bu_companies.columns:\n",
    "            raise ValueError(\"df_bu_companies must include 'bu_key'\")\n",
    "        bu_key_set = set(df_bu_companies[\"bu_key\"].astype(str))\n",
    "        # allow picking own BU with your shortlist helper (needs a 'name' col)\n",
    "        bu_names_df = pd.DataFrame({\"name\": sorted(bu_key_set)})\n",
    "\n",
    "        # build adjacency with reciprocity checks later\n",
    "        for _, r in df_bu_companies.iterrows():\n",
    "            k = str(r[\"bu_key\"])\n",
    "            sells_map[k] = _parse_refs(r.get(\"sells_to\"))\n",
    "            buys_map[k]  = _parse_refs(r.get(\"buys_from\"))\n",
    "\n",
    "    # --- helpers to build candidate pools ---\n",
    "    def _combined_partner_pool():\n",
    "        if name_column == \"product_name\":\n",
    "            base = df_customers\n",
    "        else:\n",
    "            base = df_vendors\n",
    "\n",
    "        if base is not None and not base.empty:\n",
    "            if \"name\" in base.columns:\n",
    "                base_use = base[[\"name\"]].copy()\n",
    "            elif \"customer_name\" in base.columns:\n",
    "                base_use = base.rename(columns={\"customer_name\":\"name\"})[[\"name\"]].copy()\n",
    "            elif \"vendor_name\" in base.columns:\n",
    "                base_use = base.rename(columns={\"vendor_name\":\"name\"})[[\"name\"]].copy()\n",
    "            else:\n",
    "                base_use = pd.DataFrame(columns=[\"name\"])\n",
    "        else:\n",
    "            base_use = pd.DataFrame(columns=[\"name\"])\n",
    "\n",
    "        if bu_names_df is not None and not bu_names_df.empty:\n",
    "            return pd.concat([base_use, bu_names_df], ignore_index=True).drop_duplicates(\"name\")\n",
    "        return base_use\n",
    "\n",
    "    def _allowed_bu_partners(own_bu: str) -> list[str]:\n",
    "        \"\"\"Return BU partner keys allowed by graph + reciprocity.\"\"\"\n",
    "        if not own_bu or own_bu not in bu_key_set:\n",
    "            return []\n",
    "\n",
    "        if name_column == \"product_name\":\n",
    "            # seller → choose from seller.sells_to; require reciprocal buyer.buys_from contains seller\n",
    "            candidates = sells_map.get(own_bu, set())\n",
    "            allowed = [p for p in candidates if own_bu in buys_map.get(p, set())]\n",
    "        else:\n",
    "            # buyer → choose from buyer.buys_from; require reciprocal seller.sells_to contains buyer\n",
    "            candidates = buys_map.get(own_bu, set())\n",
    "            allowed = [p for p in candidates if own_bu in sells_map.get(p, set())]\n",
    "\n",
    "        # keep only known BUs\n",
    "        return [p for p in allowed if p in bu_key_set]\n",
    "\n",
    "    def worker(item_name: str):\n",
    "        # 1) Pick account (unchanged, but use ACC_ALLOWED)\n",
    "        acc_cands = _shortlist_accounts(df_accounts, ACC_ALLOWED, item_name, k=80) or \\\n",
    "                    _shortlist_accounts(df_accounts, ACC_ALLOWED, \"\", k=80)\n",
    "\n",
    "        acc_payload = {\n",
    "            \"task\":\"Select one GL account by index\",\n",
    "            \"item_category\": name_column,\n",
    "            \"item_name\": item_name,\n",
    "            \"candidates\": acc_cands,\n",
    "            \"output_schema\": {\"account_idx\": 0},\n",
    "        }\n",
    "        acc_idx = _pick_idx_with_gpt(\n",
    "            client,\n",
    "            'Return STRICT JSON: {\"account_idx\": <int>}.',\n",
    "            acc_payload, len(acc_cands),\n",
    "            f\"{item_name}|{name_column}|account\",\n",
    "            model=model\n",
    "        )\n",
    "        acct = acc_cands[acc_idx]\n",
    "        account_name = acct[\"name\"]\n",
    "\n",
    "        # 2) Pick OWN BU first (as you had)\n",
    "        own_bu = None\n",
    "        if bu_names_df is not None and not bu_names_df.empty:\n",
    "            bu_cands = _shortlist_partners(bu_names_df, item_name, account_name, k=50)\n",
    "            if bu_cands:\n",
    "                bu_payload = {\n",
    "                    \"task\": (\"Select the SELLER BU by index\" if name_column==\"product_name\"\n",
    "                            else \"Select the BUYER BU by index\"),\n",
    "                    \"item_name\": item_name,\n",
    "                    \"account_name\": account_name,\n",
    "                    \"candidates\": bu_cands,\n",
    "                    \"output_schema\": {\"bu_idx\": 0},\n",
    "                }\n",
    "                bu_idx = _pick_idx_with_gpt(\n",
    "                    client,\n",
    "                    'Return STRICT JSON: {\"bu_idx\": <int>}.',\n",
    "                    bu_payload, len(bu_cands),\n",
    "                    f\"{item_name}|{acct['name']}|own_bu\",\n",
    "                    model=model\n",
    "                )\n",
    "                own_bu = bu_cands[bu_idx][\"name\"]\n",
    "\n",
    "        # 3) Build partner pool from BU graph; FALLBACK to combined pool if empty\n",
    "        if own_bu:\n",
    "            allowed_partner_keys = _allowed_bu_partners(own_bu)\n",
    "        else:\n",
    "            allowed_partner_keys = []\n",
    "\n",
    "        if allowed_partner_keys:\n",
    "            partner_pool = pd.DataFrame({\"name\": allowed_partner_keys})\n",
    "        else:\n",
    "            partner_pool = _combined_partner_pool()\n",
    "\n",
    "        partner_cands = _shortlist_partners(partner_pool, item_name, account_name, k=200)\n",
    "\n",
    "        # Select up to 3 distinct partners\n",
    "        rows_for_item = []\n",
    "        used_names = set()\n",
    "        picks_needed = 3\n",
    "\n",
    "        for pick_idx in range(picks_needed):\n",
    "            # filter out already used partner names\n",
    "            filtered = [c for c in partner_cands if c[\"name\"] not in used_names]\n",
    "            if not filtered:\n",
    "                break\n",
    "\n",
    "            par_payload = {\n",
    "                \"task\":\"Select one partner by index\",\n",
    "                \"item_name\": item_name,\n",
    "                \"account_name\": account_name,\n",
    "                \"candidates\": filtered,\n",
    "                \"output_schema\": {\"partner_idx\": 0},\n",
    "            }\n",
    "            pidx = _pick_idx_with_gpt(\n",
    "                client,\n",
    "                'Return STRICT JSON: {\"partner_idx\": <int>}.',\n",
    "                par_payload, len(filtered),\n",
    "                f\"{item_name}|{acct['name']}|partner|{pick_idx}\",\n",
    "                model=model\n",
    "            )\n",
    "            partner_name = filtered[pidx][\"name\"]\n",
    "            used_names.add(partner_name)\n",
    "\n",
    "            rows_for_item.append({\n",
    "                \"name\": item_name,\n",
    "                \"account_id\": acct[\"AccountKey\"],\n",
    "                \"account_name\": account_name,\n",
    "                \"customer_name\": partner_name if name_column==\"product_name\" else None,\n",
    "                \"vendor_name\":   partner_name if name_column!=\"product_name\" else None,\n",
    "                \"bu_id\":        own_bu,\n",
    "            })\n",
    "\n",
    "        # Ensure at least one row (fallback)\n",
    "        if not rows_for_item:\n",
    "            rows_for_item.append({\n",
    "                \"name\": item_name,\n",
    "                \"account_id\": acct[\"AccountKey\"],\n",
    "                \"account_name\": account_name,\n",
    "                \"customer_name\": None if name_column!=\"product_name\" else \"Unknown Customer\",\n",
    "                \"vendor_name\":   None if name_column==\"product_name\" else \"Unknown Vendor\",\n",
    "                \"bu_id\":        own_bu,\n",
    "            })\n",
    "\n",
    "        return rows_for_item\n",
    "\n",
    "    rows = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = [ex.submit(worker, str(r[\"name\"]).strip()) for _, r in df_items.iterrows()]\n",
    "        for f in as_completed(futs):\n",
    "            rows.extend(f.result())\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        rows,\n",
    "        columns=[\"name\",\"account_id\",\"account_name\",\"customer_name\",\"bu_id\", \"vendor_name\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def map_procurement_services(\n",
    "    df_procurement: pd.DataFrame,\n",
    "    df_services: pd.DataFrame,\n",
    "    df_accounts: pd.DataFrame,\n",
    "    df_customers: pd.DataFrame,\n",
    "    df_vendors: pd.DataFrame,\n",
    "    df_bu_companies: pd.DataFrame\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Combines procurement and service data into a unified spend dataset.\n",
    "\n",
    "    Returns:\n",
    "        - df_spend: Combined spend data with item_name, source_type, and *_id columns\n",
    "        - df_mapping: Combined mapping with item_name and GL/cost center info\n",
    "    \"\"\"\n",
    "\n",
    "    # Create mappings\n",
    "    procurement_mapping = create_mapping_parallel_simple(\n",
    "        df_items=df_procurement, df_accounts=df_accounts, df_customers=df_customers, df_vendors=df_vendors, df_bu_companies=df_bu_companies, name_column=\"procurement_name\"\n",
    "    )\n",
    "    print(\"✔ Procurement mapping done!\")\n",
    "    services_mapping = create_mapping_parallel_simple(\n",
    "        df_items=df_services, df_accounts=df_accounts, df_customers=df_customers, df_vendors=df_vendors, df_bu_companies=df_bu_companies, name_column=\"service_name\"\n",
    "    )\n",
    "    print(\"✔ Service mapping done!\")\n",
    "\n",
    "    # Assign ID and metadata columns\n",
    "    df_procurement = df_procurement.copy()\n",
    "    df_procurement[\"procurement_id\"] = df_procurement[\"name\"]\n",
    "    df_procurement[\"service_id\"] = None\n",
    "    df_procurement[\"product_id\"] = None\n",
    "    df_procurement[\"item_name\"] = df_procurement[\"name\"]\n",
    "    df_procurement[\"unit_price\"] = df_procurement[\"unit_price\"].fillna(0) \n",
    "    df_procurement[\"source_type\"] = \"procurement\"\n",
    "\n",
    "    df_services = df_services.copy()\n",
    "    df_services[\"service_id\"] = df_services[\"name\"]\n",
    "    df_services[\"procurement_id\"] = None\n",
    "    df_services[\"product_id\"] = None\n",
    "    df_services[\"item_name\"] = df_services[\"name\"]\n",
    "    df_services[\"unit_price\"] = df_services[\"unit_price\"].fillna(0)  \n",
    "    df_services[\"source_type\"] = \"service\"\n",
    "\n",
    "    # Combine and select columns\n",
    "    df_spend = pd.concat([df_procurement, df_services], ignore_index=True)\n",
    "    df_spend = df_spend[[\n",
    "        \"item_name\", \"source_type\", \"annual_spend\", \"unit_price\", \"proportionality\",\n",
    "        \"product_id\", \"procurement_id\", \"service_id\"\n",
    "    ]]\n",
    "\n",
    "    # Add item_name to mappings for merge compatibility\n",
    "    procurement_mapping[\"item_name\"] = procurement_mapping[\"name\"]\n",
    "    services_mapping[\"item_name\"] = services_mapping[\"name\"]\n",
    "\n",
    "    df_mapping = pd.concat([procurement_mapping, services_mapping], ignore_index=True)\n",
    "    df_mapping = df_mapping[[\"item_name\", \"account_id\", \"account_name\", \"vendor_name\", \"bu_id\"]] \n",
    "    \n",
    "    return df_spend, df_mapping\n",
    "\n",
    "def map_products(\n",
    "    df_products: pd.DataFrame,\n",
    "    df_accounts: pd.DataFrame,\n",
    "    df_customers: pd.DataFrame,\n",
    "    df_vendors: pd.DataFrame,\n",
    "    df_bu_companies: pd.DataFrame\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # Assign ID and metadata columns\n",
    "    df_products[\"product_id\"] = df_products[\"name\"]\n",
    "    df_products[\"procurement_id\"] = None\n",
    "    df_products[\"service_id\"] = None\n",
    "    df_products[\"item_name\"] = df_products[\"name\"]\n",
    "    df_products[\"source_type\"] = \"Product Sales\"\n",
    "\n",
    "    # Build spend data\n",
    "    df_spend = df_products[[\n",
    "        \"item_name\", \"source_type\", \"annual_spend\", \"unit_price\", \"proportionality\",\n",
    "        \"product_id\", \"procurement_id\", \"service_id\"\n",
    "    ]]\n",
    "\n",
    "    # Build mapping\n",
    "    df_mapping = create_mapping_parallel_simple(\n",
    "        df_items=df_products,\n",
    "        df_accounts=df_accounts,\n",
    "        df_customers=df_customers,\n",
    "        df_vendors=df_vendors,\n",
    "        df_bu_companies=df_bu_companies,\n",
    "        name_column=\"product_name\"\n",
    "    )\n",
    "\n",
    "    df_mapping[\"item_name\"] = df_mapping[\"name\"]\n",
    "    df_mapping = df_mapping[[\"item_name\", \"account_id\", \"account_name\", \"bu_id\", \"customer_name\"]]\n",
    "    return df_spend, df_mapping\n",
    "\n",
    "def remap_vendors_customers_with_bu(\n",
    "    df_customers: pd.DataFrame,\n",
    "    df_vendors: pd.DataFrame,\n",
    "    df_bu_companies: pd.DataFrame\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    \"\"\"\n",
    "    Remaps customers and vendors by adding business units (BUs) as intercompany entities.\n",
    "\n",
    "    Args:\n",
    "        df_customers (pd.DataFrame): DataFrame containing customer data.\n",
    "        df_vendors (pd.DataFrame): DataFrame containing vendor data.\n",
    "        df_bu_companies (pd.DataFrame): DataFrame containing business unit data with 'bu_key'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Updated DataFrames for customers and vendors with BUs added.\n",
    "    \"\"\"\n",
    "\n",
    "     # Create BU entries for customers and vendors\n",
    "\n",
    "    df_bu = pd.DataFrame()\n",
    "    df_bu[\"name\"] = df_bu_companies[\"bu_key\"]\n",
    "    df_bu[\"proportionality\"] = 1 / len(df_bu_companies)\n",
    "    df_bu[\"customer_segment\"] = \"Intercompany\"\n",
    "    df_bu[\"vendor_segment\"] = \"Intercompany\"\n",
    "\n",
    "    start_id_cust = int(df_customers[\"customer_id\"].max()) + 1\n",
    "    end_id_cust   = start_id_cust + len(df_bu)\n",
    "\n",
    "    start_id_vendor = int(df_vendors[\"vendor_id\"].max()) + 1\n",
    "    end_id_vendor   = start_id_vendor + len(df_bu)\n",
    "\n",
    "    df_bu[\"customer_id\"] = range(start_id_cust, end_id_cust)\n",
    "    df_bu[\"vendor_id\"] = range(start_id_vendor, end_id_vendor)\n",
    "\n",
    "    df_bu_companies[\"customer_id\"] = range(start_id_cust, end_id_cust)\n",
    "    df_bu_companies[\"vendor_id\"] = range(start_id_vendor, end_id_vendor)\n",
    "\n",
    "    df_customers = pd.concat([df_customers, df_bu.drop(columns=[\"vendor_id\", \"vendor_segment\"])], axis=0, ignore_index=True)\n",
    "    df_vendors = pd.concat([df_vendors, df_bu.drop(columns=[\"customer_id\", \"customer_segment\"])], axis=0, ignore_index=True)\n",
    "\n",
    "    df_customers = utils.convert_column_to_percentage(df_customers, \"proportionality\", scale=1.0)\n",
    "    df_vendors = utils.convert_column_to_percentage(df_vendors, \"proportionality\", scale=1.0)\n",
    "\n",
    "    return df_customers, df_vendors, df_bu_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec5d5450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modeling.erp as erp\n",
    "\n",
    "def create_mapping_between_all(generated_data: dict = None, company_name: str = None, save_to_csv: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a mapping between products, services, payroll, and GL accounts,\n",
    "    using either provided generated data or loaded CSVs from a folder.\n",
    "\n",
    "    Args:\n",
    "        generated_data (dict, optional): Dictionary containing all generated dimension tables.\n",
    "        company_name (str, optional): If provided, will load data from CSVs located at\n",
    "                                      data/outputdata/\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: (df_spend, df_mapped)\n",
    "    \"\"\"\n",
    "\n",
    "    if not generated_data and not company_name:\n",
    "        raise ValueError(\"You must provide either 'generated_data' or 'company_name'.\")\n",
    "\n",
    "    if company_name:\n",
    "        base_path = f\"data/outputdata/dimensions/\"\n",
    "        try:\n",
    "            df_products     = pd.read_csv(os.path.join(base_path, \"product.csv\"))\n",
    "            df_services     = pd.read_csv(os.path.join(base_path, \"service.csv\"))\n",
    "            df_procurement  = pd.read_csv(os.path.join(base_path, \"procurement.csv\"))\n",
    "            df_departments  = pd.read_csv(os.path.join(base_path, \"department.csv\"))\n",
    "            df_accounts     = pd.read_csv(os.path.join(base_path, \"account.csv\"))\n",
    "            df_customers    = pd.read_csv(os.path.join(base_path, \"customer.csv\"))\n",
    "            df_vendors      = pd.read_csv(os.path.join(base_path, \"vendor.csv\"))\n",
    "            df_bu_companies = pd.read_csv(os.path.join(base_path, \"bu.csv\"))\n",
    "            df_payroll      = pd.read_csv(os.path.join(\"data/outputdata/fact/\", \"erp_payroll.csv\"))\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Missing file in folder: {e.filename}\")\n",
    "    else:\n",
    "        df_products    = generated_data[\"product\"].copy()\n",
    "        df_services    = generated_data[\"service\"].copy()\n",
    "        df_procurement = generated_data[\"procurement\"].copy()\n",
    "        df_departments = generated_data[\"department_name\"].copy()\n",
    "        df_accounts    = generated_data[\"account\"].copy()\n",
    "        df_customers   = generated_data[\"customer\"].copy()\n",
    "        df_payroll     = generated_data[\"payroll\"].copy()\n",
    "        df_vendors     = generated_data[\"vendor\"].copy()\n",
    "\n",
    "    estimated_financials = erp.estimate_costs_from_payroll(df_pay=df_payroll)\n",
    "\n",
    "\n",
    "    # Apply proportional spend estimates\n",
    "    df_procurement[\"annual_spend\"] = np.round(\n",
    "        estimated_financials[\"estimated_product\"] * df_procurement[\"proportionality\"], -3\n",
    "    )\n",
    "    df_products[\"annual_spend\"] = np.round(\n",
    "        estimated_financials[\"estimated_revenue\"] * df_products[\"proportionality\"], -3\n",
    "    )\n",
    "    df_services[\"annual_spend\"] = np.round(\n",
    "        estimated_financials[\"estimated_service\"] * df_services[\"proportionality\"], -3\n",
    "    )\n",
    "    df_departments[\"annual_spend\"] = np.round(\n",
    "        estimated_financials[\"estimated_payroll\"] * df_departments[\"proportionality\"], -3\n",
    "    )\n",
    "    print(\"\\n * Semantic mapping started * :\")\n",
    "    print(\"Time estimate: 3-5 minutes\")\n",
    "    df_erp_expenses, df_map_expenses = map_procurement_services(df_procurement=df_procurement, df_services=df_services, df_accounts=df_accounts, df_customers=df_customers, df_bu_companies=df_bu_companies, df_vendors=df_vendors)\n",
    "    df_erp_products, df_map_products = map_products(df_products=df_products, df_accounts=df_accounts, df_customers=df_customers, df_bu_companies=df_bu_companies, df_vendors=df_vendors)\n",
    "\n",
    "    df_customers, df_vendors, df_bu_companies = remap_vendors_customers_with_bu(df_customers=df_customers, df_vendors=df_vendors, df_bu_companies=df_bu_companies)\n",
    "\n",
    "    list_bu_keys = df_bu_companies[\"bu_key\"].tolist()\n",
    "\n",
    "    df_map_expenses.loc[df_map_expenses[\"vendor_name\"].isin(list_bu_keys), \"account_id\"] = 4009\n",
    "    df_map_expenses.loc[df_map_expenses[\"vendor_name\"].isin(list_bu_keys), \"account_name\"] = \"Inter Company COS\"\n",
    "    df_map_products.loc[df_map_products[\"customer_name\"].isin(list_bu_keys), \"account_id\"] = 4007\n",
    "    df_map_products.loc[df_map_products[\"customer_name\"].isin(list_bu_keys), \"account_name\"] = \"Inter Company Gross Sales\"\n",
    "\n",
    "    print(f\"✔ All mapping data generated.\")\n",
    "\n",
    "\n",
    "    if save_to_csv:\n",
    "        df_customers.to_csv(f\"data/outputdata/dimensions/customer.csv\", index=False)\n",
    "        df_vendors.to_csv(f\"data/outputdata/dimensions/vendor.csv\", index=False)\n",
    "        df_bu_companies.to_csv(f\"data/outputdata/dimensions/bu.csv\", index=False)\n",
    "\n",
    "        output_dir = f\"data/outputdata/mapping\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_erp_expenses.to_csv(f\"{output_dir}/erp_expenses.csv\", index=False)\n",
    "        df_erp_products.to_csv(f\"{output_dir}/erp_products.csv\", index=False)\n",
    "        df_map_expenses.to_csv(f\"{output_dir}/map_expenses.csv\", index=False)\n",
    "        df_map_products.to_csv(f\"{output_dir}/map_products.csv\", index=False)\n",
    "        print(f\"✔ All mapping CSVs saved to: {output_dir}\")\n",
    "\n",
    "    return {\n",
    "        \"df_erp_expenses\": df_erp_expenses,\n",
    "        \"df_map_expenses\": df_map_expenses,\n",
    "        \"df_erp_products\": df_erp_products,\n",
    "        \"df_map_products\": df_map_products,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59a598fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * Semantic mapping started * :\n",
      "Time estimate: 3-5 minutes\n",
      "✔ Procurement mapping done!\n",
      "✔ Service mapping done!\n",
      "✔ All mapping data generated.\n",
      "✔ All mapping CSVs saved to: data/outputdata/mapping\n"
     ]
    }
   ],
   "source": [
    "data_mapping = create_mapping_between_all(company_name=\"BioCirc\", save_to_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_expenses = data_mapping[\"df_map_expenses\"]\n",
    "df_map_products = data_mapping[\"df_map_products\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ab91ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "account_name\n",
       "Retail COS                                   159\n",
       "Inter Company COS                            129\n",
       "Other Expenses                                36\n",
       "Repairs and Maintenance                       21\n",
       "Employee Benefits                             12\n",
       "Trade COS                                     10\n",
       "Equipment                                      6\n",
       "Utilities                                      5\n",
       "Marketing Collateral                           3\n",
       "Other Travel Related                           3\n",
       "Gains/Losses On Asset Disposal - Property      3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map_expenses[\"account_name\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customers.to_csv(f\"data/outputdata/dimensions/customer.csv\", index=False)\n",
    "        df_vendors.to_csv(f\"data/outputdata/dimensions/vendor.csv\", index=False)\n",
    "        df_bu_companies.to_csv(f\"data/outputdata/dimensions/bu.csv\", index=False)\n",
    "\n",
    "        output_dir = f\"data/outputdata/mapping\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_erp_expenses.to_csv(f\"{output_dir}/erp_expenses.csv\", index=False)\n",
    "        df_erp_products.to_csv(f\"{output_dir}/erp_products.csv\", index=False)\n",
    "        df_map_expenses.to_csv(f\"{output_dir}/map_expenses.csv\", index=False)\n",
    "        df_map_products.to_csv(f\"{output_dir}/map_products.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
