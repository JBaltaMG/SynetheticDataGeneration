{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97c01717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "from datetime import datetime as dt\n",
    "from db.db_operations import execute_db_operations\n",
    "import pandas as pd\n",
    "from utils import prompt_utils\n",
    "import numpy as np\n",
    "import utils.utils as utils\n",
    "\n",
    "from generators.full_generators import (\n",
    "    create_company_data\n",
    ")\n",
    "\n",
    "company_name = \"Lego\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "422f04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from generators.llm_generators import tailor_coa_names_llm\n",
    "\n",
    "df_accounts = pd.read_csv(\"data/inputdata/coa_general.csv\", sep = \";\")\n",
    "df_coa = tailor_coa_names_llm(df_accounts, company_name=\"Lego\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f2464a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_line_items_llm(company_name: str, \n",
    "                            count: int = 100, \n",
    "                            category_name: str = \"COGS\", \n",
    "                            financial_total: float = 100000.0, \n",
    "                            df_business_units: pd.DataFrame = pd.DataFrame(),\n",
    "                            df_parties: pd.DataFrame = pd.DataFrame(),\n",
    "                            df_accounts: pd.DataFrame = pd.DataFrame(),\n",
    "                            model: str = \"gpt-4.1\", \n",
    "                            temp: float = 0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate COGS (Cost of Goods Sold) items for a company.\n",
    "    Each item should map to Product Expense or Service Expense accounts in the COA.\n",
    "    \"\"\"\n",
    "\n",
    "    accounts_subset_csv = df_accounts.iloc[:, :2].to_csv(index=False, header=False, sep=\";\")\n",
    "    business_units_csv = df_business_units.iloc[:, :2].to_csv(index=False, header=False, sep=\";\")\n",
    "    parties_csv = df_parties.iloc[:, :3].to_csv(index=False, header=False, sep=\";\")\n",
    "\n",
    "    client = prompt_utils.get_openai_client()\n",
    "    over_request_count = int(np.floor(int(count) * 1.4))\n",
    "\n",
    "    header = \"document_number;posting_date;company;bu_id;bu_name;party_id;party_name;AccountKey;account_name;item_name;proportionality;unit_price;category\"\n",
    "    constraints = prompt_utils.get_standard_constraints(header, over_request_count)\n",
    "    ctxb = prompt_utils._ctx_block(company_name)\n",
    "\n",
    "    PROMPT_LINES = f\"\"\"\n",
    "    You are generating aggregated GL driver lines for synthetic financial data.\n",
    "    This is NOT journal entries yet. This is the template that will later be exploded\n",
    "    into many detailed postings with dates, document numbers, etc.\n",
    "\n",
    "    Company: {company_name}\n",
    "    Category to generate: {category_name}   # e.g. Revenue, COGS, FixedCost, EBIT\n",
    "    Number of rows to generate (before later down-splitting): {over_request_count}\n",
    "\n",
    "    ACCOUNTS (only use these AccountKeys for this category):\n",
    "    {accounts_subset_csv}\n",
    "\n",
    "    BUSINESS UNITS (use these bu_id values only):\n",
    "    {business_units_csv}\n",
    "\n",
    "    PARTIES (customers, vendors, internal units):\n",
    "    {parties_csv}\n",
    "\n",
    "    YOUR TASK\n",
    "    Generate {over_request_count} high-level economic driver lines for the given category {category_name}.\n",
    "\n",
    "    Each row should represent a realistic economic driver / bucket of activity\n",
    "    (e.g. \"Core LEGO set sales\", \"Licensed merchandise revenue\",\n",
    "    \"ABS resin / raw plastic pellets\", \"3PL logistics & distribution\",\n",
    "    \"Retail staff wages\", \"Store lease and rent\", \"Finance cost – long-term debt interest\").\n",
    "\n",
    "    `proportionality` rules:\n",
    "    - `proportionality` = share of total budget\n",
    "    -  Represents how large this driver is relative to the TOTAL for this category.\n",
    "\n",
    "    For each item, there must be 2-3 lines. Eg. one with intercompany, and one with an external party. Proportionality should be split accordingly.\n",
    "\n",
    "    COLUMNS AND ORDER\n",
    "    You MUST output a semicolon-separated CSV with columns in this exact order:\n",
    "\n",
    "    bu_id;party_id;AccountKey;AccountName;item_name;proportionality;category\n",
    "\n",
    "    Column definitions:\n",
    "    - bu_id:\n",
    "    - Must match one of the bu_id values from BUSINESS UNITS.\n",
    "    - Pick whichever BU is most natural for that driver (e.g. retail cost -> retail BU).\n",
    "\n",
    "    - party_id:\n",
    "    - Revenue:\n",
    "        - If AccountKey is an intercompany revenue account, party_id must be an INTERNAL_BU from PARTIES.\n",
    "        - Otherwise use a CUSTOMER from PARTIES.\n",
    "    - COGS:\n",
    "        - If AccountKey is an intercompany COGS account, party_id must be an INTERNAL_BU.\n",
    "        - Otherwise use a VENDOR from PARTIES.\n",
    "    - FixedCost:\n",
    "        - Can be blank unless there's a clear vendor/counterparty (e.g. \"External legal services\").\n",
    "    - EBIT (other income/expense categories):\n",
    "        - Can be blank unless it's obviously a financing/royalty counterparty.\n",
    "\n",
    "    - AccountKey:\n",
    "    - Must be copied from the provided ACCOUNTS list.\n",
    "    - Only use AccountKeys valid for this category:\n",
    "        - Revenue  → 4001–4009\n",
    "        - COGS     → 4003, 4006, 4009\n",
    "        - FixedCost→ 5001–5027\n",
    "        - EBIT     → 6001–6503\n",
    "        - BalanceSheet → 1001–3005\n",
    "\n",
    "    - AccountName:\n",
    "    - Must be copied from 'name' in the accounts list for that AccountKey.\n",
    "\n",
    "    - item_name:\n",
    "    - MUST be generic category-level spend/revenue drivers.\n",
    "    - MUST NOT include dates, months, regions, shipment references, batch IDs, PO numbers, 'January', 'Copenhagen', etc.\n",
    "    - MUST NOT include \"for\" phrases or detailed invoice descriptions.\n",
    "    - GOOD: \"Plastic gloves\". \n",
    "\n",
    "    - proportionality:\n",
    "    - Decimal between 0 and 1.\n",
    "    - All proportionality values across all rows MUST sum to 1.0 total.\n",
    "    - Represents relative size of this driver within the category.\n",
    "    \n",
    "    - unit_price:\n",
    "    - unit_price of the item in DKK.\n",
    "\n",
    "    - category:\n",
    "    - Must equal {category_name} exactly, for every row.\n",
    "\n",
    "    FINAL OUTPUT RULES\n",
    "    - Output ONLY CSV rows, one row per driver line.\n",
    "    - Use semicolons as separators.\n",
    "    - Do NOT include headers.\n",
    "    - Do NOT include document_number.\n",
    "    - Do NOT include amount_DKK.\n",
    "    - Do NOT include unit_price.\n",
    "    - Do NOT include explanations, notes, or markdown fences.\n",
    "\n",
    "    {constraints}\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful financial analyst and ERP mapping assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": PROMPT_LINES},\n",
    "        ],\n",
    "        temperature=temp,\n",
    "    )\n",
    "\n",
    "    df_cogs = prompt_utils.parse_and_truncate_csv(response.choices[0].message.content, count)\n",
    "    df_cogs = utils.convert_column_to_percentage(df_cogs, \"proportionality\", scale=1.0)\n",
    "    df_cogs[\"annual_spend\"] = np.round(df_cogs[\"proportionality\"] * financial_total, -2)\n",
    "    return df_cogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "986a21e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bus_llm(company_name: str, count: int = 100, model: str = \"gpt-4.1\", temp: float = 0.8):\n",
    "    client = prompt_utils.get_openai_client()\n",
    "    over_request_count = int(np.floor(int(count) * 1.4))\n",
    "    header = \"bu_id;bu_name;bu_type;department;country\"\n",
    "    constraints = prompt_utils.get_standard_constraints(header, over_request_count)\n",
    "    ctxb = prompt_utils._ctx_block(company_name)\n",
    "\n",
    "    PROMPT_BUSINESS_UNITS = f\"\"\"\n",
    "    You are creating a realistic internal org structure for a company.\n",
    "\n",
    "    Company: {company_name}\n",
    "\n",
    "    Task:\n",
    "    Generate 10-15 business units and departments that reflect how this company would actually operate (production sites, regional sales orgs, HQ functions, logistics hubs, shared services, etc.).\n",
    "\n",
    "    Return ONLY a semicolon-separated CSV with the following columns in this exact order:\n",
    "    {header}\n",
    "\n",
    "    Definitions:\n",
    "    - BU_ID: stable ID like BU001, BU002, ...\n",
    "    - BU_Name: human label, e.g. \"LEGO Retail DK\", \"LEGO Factory CZ\", \"LEGO HQ Billund\"\n",
    "    - BU_Type: one of [Factory, Retail, HQ, Licensing, Shared Service, Online, Distribution]\n",
    "    - Department: e.g. \"Sales\", \"Manufacturing\", \"Finance\", \"Marketing\", \"Logistics\", \"IT\"\n",
    "    - Country: realistic country/region for that BU\n",
    "    - Description: short purpose of this unit\n",
    "\n",
    "    Rules:\n",
    "    - Make sure there is at least one HQ / corporate finance unit.\n",
    "    - Make sure there are both commercial (sales/retail) and production/supply-side units.\n",
    "    - IDs must be unique.\n",
    "    {constraints}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful data assistant and B2B vendor segmentation expert.\"},\n",
    "                  {\"role\": \"user\", \"content\": PROMPT_BUSINESS_UNITS}],\n",
    "        temperature=temp,\n",
    "    )\n",
    "    \n",
    "    return prompt_utils.parse_and_truncate_csv(response.choices[0].message.content, count)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e72399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parties(company_name: str, \n",
    "                            count: int = 100, \n",
    "                            df_business_units: pd.DataFrame = pd.DataFrame(),\n",
    "                            model: str = \"gpt-5\", \n",
    "                            temp: float = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate COGS (Cost of Goods Sold) items for a company.\n",
    "    Each item should map to Product Expense or Service Expense accounts in the COA.\n",
    "    \"\"\"\n",
    "    business_units_csv = df_business_units.iloc[:, :2].to_csv(index=False, header=False, sep=\";\")\n",
    "    \n",
    "    client = prompt_utils.get_openai_client()\n",
    "    over_request_count = int(np.floor(int(count) * 1.4))\n",
    "    \n",
    "    header = \"party_ID;party_name;party_type\"\n",
    "    constraints = prompt_utils.get_standard_constraints(header, over_request_count)\n",
    "    ctxb = prompt_utils._ctx_block(company_name)\n",
    "\n",
    "    PROMPT_PARTIES = f\"\"\"\n",
    "    You are creating master data for all counterparties in this company.\n",
    "\n",
    "    Company: {company_name}\n",
    "\n",
    "    Internal business units (BU master data):\n",
    "    {business_units_csv}\n",
    "\n",
    "    Task:\n",
    "    1. For each internal BU, create a row where that BU is treated as a party.\n",
    "    2. Also create external customers (distributors, retailers, channels).\n",
    "    3. Also create external vendors (materials suppliers, logistics, energy, maintenance, IT services).\n",
    "\n",
    "    Generate {over_request_count} rows TOTAL across all types.\n",
    "\n",
    "    Return ONLY a semicolon-separated CSV with columns in this exact order:\n",
    "    {header}\n",
    "\n",
    "    Where:\n",
    "    - party_ID:\n",
    "    - INTERNAL_BU => \"INT###\"\n",
    "    - CUSTOMER    => \"CUST###\"\n",
    "    - VENDOR      => \"VEND###\"\n",
    "    - party_Type is exactly one of [INTERNAL_BU, CUSTOMER, VENDOR]\n",
    "    - INTERNAL_BU rows must include ALL internal business units given above.\n",
    "    - party_Name for INTERNAL_BU must match BU_Name exactly.\n",
    "    - No commentary, no markdown, only CSV.\n",
    "\n",
    "    {constraints}\n",
    "\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful financial analyst and ERP mapping assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": PROMPT_PARTIES},\n",
    "        ],\n",
    "        temperature=temp,\n",
    "    )\n",
    "\n",
    "    df_parties = prompt_utils.parse_and_truncate_csv(response.choices[0].message.content, count)\n",
    "\n",
    "    return df_parties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3324fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bus = generate_bus_llm(company_name=company_name, model=\"gpt-4.1\", temp=0.7, count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "136b44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parties = generate_parties(company_name=company_name, model=\"gpt-4.1\", temp=0.7, count=50, df_business_units=df_bus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee128279",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cogs = generate_line_items_llm(company_name=company_name, \n",
    "                                  count=200, \n",
    "                                  category_name=\"COGS\", \n",
    "                                  financial_total=50000000.0, \n",
    "                                  df_business_units=df_bus,\n",
    "                                  df_parties=df_parties,\n",
    "                                  df_accounts=df_coa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65731fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ebit = generate_line_items_llm(company_name=company_name, \n",
    "                                  count=70, \n",
    "                                  category_name=\"EBIT\", \n",
    "                                  financial_total=500000.0, \n",
    "                                  df_business_units=df_bus,\n",
    "                                  df_parties=df_parties,\n",
    "                                  df_accounts=df_coa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d52616c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revenue = generate_line_items_llm(company_name=company_name,\n",
    "                                    count=200, \n",
    "                                    category_name=\"Revenue\", \n",
    "                                    financial_total=120000000.0, \n",
    "                                    df_business_units=df_bus,\n",
    "                                    df_parties=df_parties,\n",
    "                                    df_accounts=df_coa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32698864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balance = generate_line_items_llm(company_name=company_name,\n",
    "                                    count=150,\n",
    "                                    category_name=\"BalanceSheet\",\n",
    "                                    financial_total=80000000.0,\n",
    "                                    df_business_units=df_bus,\n",
    "                                    df_parties=df_parties,\n",
    "                                    df_accounts=df_coa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dda43d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_number</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>company</th>\n",
       "      <th>bu_id</th>\n",
       "      <th>bu_name</th>\n",
       "      <th>party_id</th>\n",
       "      <th>party_name</th>\n",
       "      <th>AccountKey</th>\n",
       "      <th>account_name</th>\n",
       "      <th>item_name</th>\n",
       "      <th>proportionality</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>category</th>\n",
       "      <th>annual_spend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU003</td>\n",
       "      <td>LEGO Factory Billund</td>\n",
       "      <td>VEND001</td>\n",
       "      <td>BASF SE</td>\n",
       "      <td>1003</td>\n",
       "      <td>Raw Materials for Brick Production</td>\n",
       "      <td>ABS resin raw materials</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>10500</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>12400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU003</td>\n",
       "      <td>LEGO Factory Billund</td>\n",
       "      <td>VEND002</td>\n",
       "      <td>Covestro AG</td>\n",
       "      <td>1003</td>\n",
       "      <td>Raw Materials for Brick Production</td>\n",
       "      <td>Colorant additives</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>3700</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU006</td>\n",
       "      <td>LEGO Factory Jiaxing</td>\n",
       "      <td>VEND004</td>\n",
       "      <td>Braskem S.A.</td>\n",
       "      <td>1003</td>\n",
       "      <td>Raw Materials for Brick Production</td>\n",
       "      <td>Plant-based plastic pellets</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>8400</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>8100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU007</td>\n",
       "      <td>LEGO Factory Monterrey</td>\n",
       "      <td>VEND030</td>\n",
       "      <td>Jiangsu Sanxin Plastic</td>\n",
       "      <td>1003</td>\n",
       "      <td>Raw Materials for Brick Production</td>\n",
       "      <td>Injection molding granules</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>7700</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>8700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU006</td>\n",
       "      <td>LEGO Factory Jiaxing</td>\n",
       "      <td>VEND025</td>\n",
       "      <td>ISS Facility Services China</td>\n",
       "      <td>1004</td>\n",
       "      <td>LEGO Sets in Assembly</td>\n",
       "      <td>Assembly support services</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>1300</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>4700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU011</td>\n",
       "      <td>LEGO Retail Germany</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>Bank and Cash Balances</td>\n",
       "      <td>Retail cash</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>671100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU012</td>\n",
       "      <td>LEGO Retail USA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>Bank and Cash Balances</td>\n",
       "      <td>Retail cash</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>671100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU017</td>\n",
       "      <td>LEGO Group Marketing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>Bank and Cash Balances</td>\n",
       "      <td>Marketing cash</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>671100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU015</td>\n",
       "      <td>LEGO Licensing EMEA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>Bank and Cash Balances</td>\n",
       "      <td>Licensing cash</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>671100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU016</td>\n",
       "      <td>LEGO Licensing APAC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001</td>\n",
       "      <td>Bank and Cash Balances</td>\n",
       "      <td>Licensing cash</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.008389</td>\n",
       "      <td>BalanceSheet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>671100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_number  posting_date company  bu_id                 bu_name  \\\n",
       "0                NaN           NaN    Lego  BU003    LEGO Factory Billund   \n",
       "1                NaN           NaN    Lego  BU003    LEGO Factory Billund   \n",
       "2                NaN           NaN    Lego  BU006    LEGO Factory Jiaxing   \n",
       "3                NaN           NaN    Lego  BU007  LEGO Factory Monterrey   \n",
       "4                NaN           NaN    Lego  BU006    LEGO Factory Jiaxing   \n",
       "..               ...           ...     ...    ...                     ...   \n",
       "145              NaN           NaN    Lego  BU011     LEGO Retail Germany   \n",
       "146              NaN           NaN    Lego  BU012         LEGO Retail USA   \n",
       "147              NaN           NaN    Lego  BU017    LEGO Group Marketing   \n",
       "148              NaN           NaN    Lego  BU015     LEGO Licensing EMEA   \n",
       "149              NaN           NaN    Lego  BU016     LEGO Licensing APAC   \n",
       "\n",
       "    party_id                   party_name              AccountKey  \\\n",
       "0    VEND001                      BASF SE                    1003   \n",
       "1    VEND002                  Covestro AG                    1003   \n",
       "2    VEND004                 Braskem S.A.                    1003   \n",
       "3    VEND030       Jiangsu Sanxin Plastic                    1003   \n",
       "4    VEND025  ISS Facility Services China                    1004   \n",
       "..       ...                          ...                     ...   \n",
       "145      NaN                         1001  Bank and Cash Balances   \n",
       "146      NaN                         1001  Bank and Cash Balances   \n",
       "147      NaN                         1001  Bank and Cash Balances   \n",
       "148      NaN                         1001  Bank and Cash Balances   \n",
       "149      NaN                         1001  Bank and Cash Balances   \n",
       "\n",
       "                           account_name                    item_name  \\\n",
       "0    Raw Materials for Brick Production      ABS resin raw materials   \n",
       "1    Raw Materials for Brick Production           Colorant additives   \n",
       "2    Raw Materials for Brick Production  Plant-based plastic pellets   \n",
       "3    Raw Materials for Brick Production   Injection molding granules   \n",
       "4                 LEGO Sets in Assembly    Assembly support services   \n",
       "..                                  ...                          ...   \n",
       "145                         Retail cash                        0.003   \n",
       "146                         Retail cash                        0.004   \n",
       "147                      Marketing cash                        0.001   \n",
       "148                      Licensing cash                        0.001   \n",
       "149                      Licensing cash                        0.001   \n",
       "\n",
       "     proportionality    unit_price      category  annual_spend  \n",
       "0           0.000155         10500  BalanceSheet       12400.0  \n",
       "1           0.000076          3700  BalanceSheet        6000.0  \n",
       "2           0.000101          8400  BalanceSheet        8100.0  \n",
       "3           0.000109          7700  BalanceSheet        8700.0  \n",
       "4           0.000059          1300  BalanceSheet        4700.0  \n",
       "..               ...           ...           ...           ...  \n",
       "145         0.008389  BalanceSheet           NaN      671100.0  \n",
       "146         0.008389  BalanceSheet           NaN      671100.0  \n",
       "147         0.008389  BalanceSheet           NaN      671100.0  \n",
       "148         0.008389  BalanceSheet           NaN      671100.0  \n",
       "149         0.008389  BalanceSheet           NaN      671100.0  \n",
       "\n",
       "[150 rows x 14 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "538a7f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_number</th>\n",
       "      <th>posting_date</th>\n",
       "      <th>company</th>\n",
       "      <th>bu_id</th>\n",
       "      <th>bu_name</th>\n",
       "      <th>party_id</th>\n",
       "      <th>party_name</th>\n",
       "      <th>AccountKey</th>\n",
       "      <th>account_name</th>\n",
       "      <th>item_name</th>\n",
       "      <th>proportionality</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>category</th>\n",
       "      <th>annual_spend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU012</td>\n",
       "      <td>LEGO Retail USA</td>\n",
       "      <td>CUST017</td>\n",
       "      <td>Barnes &amp; Noble USA</td>\n",
       "      <td>4001</td>\n",
       "      <td>Retail Sales Revenue</td>\n",
       "      <td>Core LEGO set sales - retail channel</td>\n",
       "      <td>0.038688</td>\n",
       "      <td>299</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>4642600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU012</td>\n",
       "      <td>LEGO Retail USA</td>\n",
       "      <td>CUST003</td>\n",
       "      <td>Target Corporation</td>\n",
       "      <td>4001</td>\n",
       "      <td>Retail Sales Revenue</td>\n",
       "      <td>Core LEGO set sales - retail channel</td>\n",
       "      <td>0.032269</td>\n",
       "      <td>299</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>3872300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU011</td>\n",
       "      <td>LEGO Retail Germany</td>\n",
       "      <td>CUST026</td>\n",
       "      <td>Fnac Darty France</td>\n",
       "      <td>4001</td>\n",
       "      <td>Retail Sales Revenue</td>\n",
       "      <td>Core LEGO set sales - retail channel</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>289</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>3497600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU010</td>\n",
       "      <td>LEGO Retail UK</td>\n",
       "      <td>CUST014</td>\n",
       "      <td>Hamleys UK</td>\n",
       "      <td>4001</td>\n",
       "      <td>Retail Sales Revenue</td>\n",
       "      <td>Core LEGO set sales - retail channel</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>299</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>2956300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU010</td>\n",
       "      <td>LEGO Retail UK</td>\n",
       "      <td>CUST030</td>\n",
       "      <td>Argos UK</td>\n",
       "      <td>4001</td>\n",
       "      <td>Retail Sales Revenue</td>\n",
       "      <td>Core LEGO set sales - retail channel</td>\n",
       "      <td>0.023768</td>\n",
       "      <td>299</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>2852200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU001</td>\n",
       "      <td>LEGO HQ Billund</td>\n",
       "      <td>CUST012</td>\n",
       "      <td>Best Buy USA</td>\n",
       "      <td>4001</td>\n",
       "      <td>Retail Sales Revenue</td>\n",
       "      <td>Employee store sales</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>199</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>41600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU001</td>\n",
       "      <td>LEGO HQ Billund</td>\n",
       "      <td>CUST002</td>\n",
       "      <td>Walmart Inc.</td>\n",
       "      <td>4002</td>\n",
       "      <td>Retail Sales Discounts</td>\n",
       "      <td>Employee store sales discounts</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-19</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>20800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU001</td>\n",
       "      <td>LEGO HQ Billund</td>\n",
       "      <td>CUST003</td>\n",
       "      <td>Target Corporation</td>\n",
       "      <td>4002</td>\n",
       "      <td>Retail Sales Discounts</td>\n",
       "      <td>Employee store sales discounts</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-19</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>20800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU001</td>\n",
       "      <td>LEGO HQ Billund</td>\n",
       "      <td>CUST012</td>\n",
       "      <td>Best Buy USA</td>\n",
       "      <td>4002</td>\n",
       "      <td>Retail Sales Discounts</td>\n",
       "      <td>Employee store sales discounts</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-19</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>20800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lego</td>\n",
       "      <td>BU001</td>\n",
       "      <td>LEGO HQ Billund</td>\n",
       "      <td>CUST017</td>\n",
       "      <td>Barnes &amp; Noble USA</td>\n",
       "      <td>4002</td>\n",
       "      <td>Retail Sales Discounts</td>\n",
       "      <td>Employee store sales discounts</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-19</td>\n",
       "      <td>Revenue</td>\n",
       "      <td>20800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     document_number  posting_date company  bu_id              bu_name  \\\n",
       "0                NaN           NaN    Lego  BU012      LEGO Retail USA   \n",
       "1                NaN           NaN    Lego  BU012      LEGO Retail USA   \n",
       "2                NaN           NaN    Lego  BU011  LEGO Retail Germany   \n",
       "3                NaN           NaN    Lego  BU010       LEGO Retail UK   \n",
       "4                NaN           NaN    Lego  BU010       LEGO Retail UK   \n",
       "..               ...           ...     ...    ...                  ...   \n",
       "195              NaN           NaN    Lego  BU001      LEGO HQ Billund   \n",
       "196              NaN           NaN    Lego  BU001      LEGO HQ Billund   \n",
       "197              NaN           NaN    Lego  BU001      LEGO HQ Billund   \n",
       "198              NaN           NaN    Lego  BU001      LEGO HQ Billund   \n",
       "199              NaN           NaN    Lego  BU001      LEGO HQ Billund   \n",
       "\n",
       "    party_id          party_name  AccountKey            account_name  \\\n",
       "0    CUST017  Barnes & Noble USA        4001    Retail Sales Revenue   \n",
       "1    CUST003  Target Corporation        4001    Retail Sales Revenue   \n",
       "2    CUST026   Fnac Darty France        4001    Retail Sales Revenue   \n",
       "3    CUST014          Hamleys UK        4001    Retail Sales Revenue   \n",
       "4    CUST030            Argos UK        4001    Retail Sales Revenue   \n",
       "..       ...                 ...         ...                     ...   \n",
       "195  CUST012        Best Buy USA        4001    Retail Sales Revenue   \n",
       "196  CUST002        Walmart Inc.        4002  Retail Sales Discounts   \n",
       "197  CUST003  Target Corporation        4002  Retail Sales Discounts   \n",
       "198  CUST012        Best Buy USA        4002  Retail Sales Discounts   \n",
       "199  CUST017  Barnes & Noble USA        4002  Retail Sales Discounts   \n",
       "\n",
       "                                item_name  proportionality  unit_price  \\\n",
       "0    Core LEGO set sales - retail channel         0.038688         299   \n",
       "1    Core LEGO set sales - retail channel         0.032269         299   \n",
       "2    Core LEGO set sales - retail channel         0.029146         289   \n",
       "3    Core LEGO set sales - retail channel         0.024636         299   \n",
       "4    Core LEGO set sales - retail channel         0.023768         299   \n",
       "..                                    ...              ...         ...   \n",
       "195                  Employee store sales         0.000347         199   \n",
       "196        Employee store sales discounts         0.000173         -19   \n",
       "197        Employee store sales discounts         0.000173         -19   \n",
       "198        Employee store sales discounts         0.000173         -19   \n",
       "199        Employee store sales discounts         0.000173         -19   \n",
       "\n",
       "    category  annual_spend  \n",
       "0    Revenue     4642600.0  \n",
       "1    Revenue     3872300.0  \n",
       "2    Revenue     3497600.0  \n",
       "3    Revenue     2956300.0  \n",
       "4    Revenue     2852200.0  \n",
       "..       ...           ...  \n",
       "195  Revenue       41600.0  \n",
       "196  Revenue       20800.0  \n",
       "197  Revenue       20800.0  \n",
       "198  Revenue       20800.0  \n",
       "199  Revenue       20800.0  \n",
       "\n",
       "[200 rows x 14 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5379cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1) Schema & normalization\n",
    "# =============================================================================\n",
    "\n",
    "erp_cols = {\n",
    "    \"document_number\": \"document_number\",\n",
    "    \"debit_credit\": \"debit_credit\",\n",
    "    \"date\": \"date\",\n",
    "    \"amount\": \"amount\",\n",
    "    \"quantity\": \"quantity\",\n",
    "    \"account_name\": \"account_name\",\n",
    "    \"product_id\": \"product_id\",\n",
    "    \"procurement_id\": \"procurement_id\",\n",
    "    \"service_id\": \"service_id\",\n",
    "    \"vendor_name\": \"vendor_name\",\n",
    "    \"customer_name\": \"customer_name\",\n",
    "}\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Schema:\n",
    "    mapping: Dict[str, str]\n",
    "\n",
    "    @property\n",
    "    def inverse(self) -> Dict[str, str]:\n",
    "        return {v: k for k, v in self.mapping.items()}\n",
    "\n",
    "    # canonical names (helpers to avoid string literals all over)\n",
    "    @property\n",
    "    def document_number(self): return \"document_number\"\n",
    "    @property\n",
    "    def debit_credit(self):    return \"debit_credit\"\n",
    "    @property\n",
    "    def date(self):            return \"date\"\n",
    "    @property\n",
    "    def amount(self):          return \"amount\"\n",
    "    @property\n",
    "    def quantity(self):        return \"quantity\"\n",
    "    @property\n",
    "    def account_id(self):      return \"account_name\"\n",
    "    @property\n",
    "    def product_id(self):      return \"product_id\"\n",
    "    @property\n",
    "    def procurement_id(self):  return \"procurement_id\"\n",
    "    @property\n",
    "    def service_id(self):      return \"service_id\"\n",
    "    @property\n",
    "    def vendor_id(self):       return \"vendor_name\"\n",
    "    @property\n",
    "    def customer_id(self):     return \"customer_name\"\n",
    "\n",
    "S = Schema(mapping=erp_cols)\n",
    "\n",
    "\n",
    "def normalize_df(df: pd.DataFrame, schema: Schema = S) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rename raw/source → canonical, but *skip* a rename if it would\n",
    "    overwrite an existing canonical column (avoids duplicate labels).\n",
    "    \"\"\"\n",
    "    ren = {}\n",
    "    for raw, canon in schema.mapping.items():\n",
    "        if raw in df.columns:\n",
    "            # If target canonical already exists, skip this rename\n",
    "            # (e.g., raw='account_name', canon='account_id' while 'account_id' already present)\n",
    "            if canon in df.columns and canon != raw:\n",
    "                continue\n",
    "            ren[raw] = canon\n",
    "    return df.rename(columns=ren)\n",
    "\n",
    "\n",
    "def denormalize_df(df: pd.DataFrame, schema: Schema = S) -> pd.DataFrame:\n",
    "    \"\"\"Rename canonical -> original raw names (if needed).\"\"\"\n",
    "    inv = schema.inverse\n",
    "    return df.rename(columns={k: v for k, v in inv.items() if k in df.columns})\n",
    "\n",
    "\n",
    "def coalesce_duplicate_named_columns(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If df has multiple columns named 'name', merge them left→right (first non-null),\n",
    "    keep a single column.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # find *all* columns with exact same label\n",
    "    idx = np.where(np.array(df.columns) == name)[0]\n",
    "    if len(idx) <= 1:\n",
    "        return df\n",
    "\n",
    "    merged = df.iloc[:, idx].bfill(axis=1).iloc[:, 0]\n",
    "    out = df.drop(df.columns[idx[1:]], axis=1).copy()\n",
    "    out[name] = merged\n",
    "    return out\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Date helpers (fast & simple)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_dim_date(\n",
    "    year_start: int = 2020,\n",
    "    year_end: int = 2025,\n",
    "    business_days_only: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create a simple date dimension with a 'date' column.\"\"\"\n",
    "    start = f\"{year_start}-01-01\"\n",
    "    end = f\"{year_end}-12-31\"\n",
    "    if business_days_only:\n",
    "        dates = pd.bdate_range(start, end)\n",
    "    else:\n",
    "        dates = pd.date_range(start, end, freq=\"D\")\n",
    "    return pd.DataFrame({\"date\": pd.to_datetime(dates)})\n",
    "\n",
    "\n",
    "def sample_dates(\n",
    "    df_date: pd.DataFrame,\n",
    "    size: int,\n",
    "    *,\n",
    "    ensure_quarter_balance: bool = False,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Fast date sampler. If ensure_quarter_balance=True, tries to pick ~equal\n",
    "    amount from each quarter. Otherwise, uniform sampling.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(123)\n",
    "    arr = pd.to_datetime(df_date[\"date\"]).to_numpy()\n",
    "    if size <= 0:\n",
    "        return np.array([], dtype=\"datetime64[ns]\")\n",
    "\n",
    "    if not ensure_quarter_balance:\n",
    "        return rng.choice(arr, size=size, replace=True)\n",
    "\n",
    "    # Stratify by quarter (approx equal per quarter)\n",
    "    df = pd.DataFrame({\"date\": arr})\n",
    "    q = pd.PeriodIndex(df[\"date\"], freq=\"Q\")\n",
    "    groups = {k: v[\"date\"].to_numpy() for k, v in df.groupby(q)}\n",
    "\n",
    "    # compute roughly equal allocation\n",
    "    n_quarters = max(1, len(groups))\n",
    "    base = size // n_quarters\n",
    "    rem = size - base * n_quarters\n",
    "\n",
    "    picks: list[np.ndarray] = []\n",
    "    keys = list(groups.keys())\n",
    "    for i, key in enumerate(keys):\n",
    "        ksz = base + (1 if i < rem else 0)\n",
    "        pool = groups[key]\n",
    "        if pool.size == 0:\n",
    "            continue\n",
    "        picks.append(rng.choice(pool, size=ksz, replace=True))\n",
    "\n",
    "    if len(picks) == 0:\n",
    "        return rng.choice(arr, size=size, replace=True)\n",
    "\n",
    "    out = np.concatenate(picks)\n",
    "    # shuffle for randomness\n",
    "    rng.shuffle(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) Target doc value helper (adaptive realism from unit_price)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_target_doc_value_per_item(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    unit_price_col: str = \"unit_price\",\n",
    "    max_lines_per_doc: int = 50,\n",
    "    cap_factor: float = 1.15,\n",
    "    min_doc_value: float = 10_000,\n",
    "    cheap_expensive_log10: tuple[float, float] = (3.0, 6.0),  # 10^3..10^6 price band\n",
    "    lines_for_band: tuple[int, int] = (20, 4),                # cheap→20 lines, expensive→4 lines\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - desired_lines (int),\n",
    "      - target_doc_value (float),\n",
    "      - doc_value_cap (float)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    u = pd.to_numeric(out[unit_price_col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).fillna(1.0).clip(lower=1.0)\n",
    "    logu = np.log10(u)\n",
    "\n",
    "    lo, hi = cheap_expensive_log10\n",
    "    l_lo, l_hi = lines_for_band\n",
    "    desired_lines = np.interp(logu, [lo, hi], [l_lo, l_hi])\n",
    "    desired_lines = np.clip(np.round(desired_lines), 1, max_lines_per_doc).astype(int)\n",
    "\n",
    "    target_doc_value = u * desired_lines\n",
    "    doc_value_cap = cap_factor * u * max_lines_per_doc\n",
    "\n",
    "    target_doc_value = np.clip(target_doc_value, min_doc_value, doc_value_cap)\n",
    "\n",
    "    out[\"desired_lines\"] = desired_lines\n",
    "    out[\"target_doc_value\"] = np.round(target_doc_value, 2)\n",
    "    out[\"doc_value_cap\"] = np.round(doc_value_cap, 2)\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4) Fast: assign number of docs & lines (vectorized)\n",
    "# =============================================================================\n",
    "\n",
    "def fast_assign_counts(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    max_lines_per_doc: int = 25,\n",
    "    cap_factor: float = 1.15,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    "    lines_for_band: tuple[int, int] = (12, 2),  # cheap→12 lines, expensive→2 lines\n",
    "    total_cols: Iterable[str] = (\"total_amount\", \"annual_spend\", \"total_ammount\", \"total\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      - n_docs\n",
    "      - avg_lines_per_doc\n",
    "      - doc_value_cap\n",
    "      - target_doc_value (if not already present)\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # Pick a total column\n",
    "    amt_col = next((c for c in total_cols if c in out.columns), None)\n",
    "    if amt_col is None:\n",
    "        raise ValueError(\"Need a total amount column (e.g., 'annual_spend').\")\n",
    "    if \"unit_price\" not in out.columns:\n",
    "        raise ValueError(\"Missing 'unit_price'.\")\n",
    "\n",
    "    total = pd.to_numeric(out[amt_col], errors=\"coerce\").fillna(0.0).clip(lower=0.0).to_numpy()\n",
    "    unit  = pd.to_numeric(out[\"unit_price\"], errors=\"coerce\").replace([np.inf,-np.inf], np.nan).fillna(1.0).clip(lower=1.0).to_numpy()\n",
    "\n",
    "    # doc cap\n",
    "    doc_cap = cap_factor * unit * max_lines_per_doc\n",
    "\n",
    "    # target doc value (use existing if present, else compute adaptive)\n",
    "    if \"target_doc_value\" in out.columns:\n",
    "        tgt = pd.to_numeric(out[\"target_doc_value\"], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "        tgt = tgt.to_numpy()\n",
    "        # sanitize & clip\n",
    "        tgt = np.where(np.isfinite(tgt) & (tgt > 0), tgt, np.nan)\n",
    "    else:\n",
    "        tmp = compute_target_doc_value_per_item(out, max_lines_per_doc=max_lines_per_doc, lines_for_band=lines_for_band,cap_factor=cap_factor)\n",
    "        out[[\"desired_lines\", \"target_doc_value\", \"doc_value_cap\"]] = tmp[[\"desired_lines\", \"target_doc_value\", \"doc_value_cap\"]]\n",
    "        tgt = out[\"target_doc_value\"].to_numpy()\n",
    "\n",
    "    tgt = np.clip(np.nan_to_num(tgt, nan=(doc_cap / 2.0)), 1.0, doc_cap)\n",
    "\n",
    "    # docs: ceil(total / tgt); feasibility: at least total/doc_cap\n",
    "    base_docs = np.ceil(np.divide(total, tgt, out=np.zeros_like(total), where=tgt > 0)).astype(int)\n",
    "    min_docs  = np.ceil(np.divide(total, doc_cap, out=np.ones_like(total), where=doc_cap > 0)).astype(int)\n",
    "    n_docs    = np.maximum(base_docs, min_docs)\n",
    "    n_docs    = np.clip(n_docs, 1, None)\n",
    "\n",
    "    # avg lines per doc: ceil(total units / n_docs), bounded\n",
    "    est_total_lines = np.ceil(total / unit).astype(int)\n",
    "    est_total_lines[~np.isfinite(est_total_lines)] = 1\n",
    "    est_total_lines = np.clip(est_total_lines, 1, None)\n",
    "\n",
    "    avg_lines_per_doc = np.ceil(est_total_lines / n_docs).astype(int)\n",
    "    avg_lines_per_doc = np.clip(avg_lines_per_doc, 1, max_lines_per_doc)\n",
    "\n",
    "    out[\"n_docs\"] = n_docs\n",
    "    out[\"avg_lines_per_doc\"] = avg_lines_per_doc\n",
    "    out[\"doc_value_cap\"] = np.round(doc_cap, 2)\n",
    "    # ensure target_doc_value is present\n",
    "    if \"target_doc_value\" not in out.columns:\n",
    "        out[\"target_doc_value\"] = np.round(tgt, 2)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5) Fast: explode into documents (multinomial cents split)\n",
    "# =============================================================================\n",
    "\n",
    "def fast_make_doc_plan(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    rng=None,\n",
    "    df_date=None,\n",
    "    ensure_quarter_balance=True,\n",
    "    total_cols=(\"total_amount\",\"annual_spend\",\"total_ammount\",\"total\"),\n",
    "    target_qty_per_line: float = 20.0,\n",
    "    qty_sigma: float = 0.6,        # ← NEW: variability for target qty (lognormal)\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(43)\n",
    "    \"\"\"\n",
    "    Returns a per-document plan with:\n",
    "      item_name, source_type, date, doc_value, n_lines, unit_price, IDs...\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(43)\n",
    "\n",
    "    # pick totals\n",
    "    amt_col = next((c for c in total_cols if c in df.columns), None)\n",
    "    if amt_col is None:\n",
    "        raise ValueError(\"Need a total amount column (e.g., 'annual_spend').\")\n",
    "    if \"n_docs\" not in df.columns or \"avg_lines_per_doc\" not in df.columns:\n",
    "        raise ValueError(\"Run fast_assign_counts first.\")\n",
    "\n",
    "    n_docs = df[\"n_docs\"].to_numpy().astype(int)\n",
    "    idx_rep = np.repeat(np.arange(len(df)), n_docs)\n",
    "    plan = df.iloc[idx_rep].copy()\n",
    "\n",
    "    # multinomial cents split per item\n",
    "    totals_cents = (pd.to_numeric(df[amt_col], errors=\"coerce\").fillna(0.0).clip(lower=0.0).to_numpy() * 100).astype(int)\n",
    "    cents_blocks = [\n",
    "        rng.multinomial(t, np.ones(nd) / nd) if nd > 0 else np.zeros(1, dtype=int)\n",
    "        for t, nd in zip(totals_cents, n_docs)\n",
    "    ]\n",
    "    doc_cents = np.concatenate(cents_blocks)\n",
    "    plan[\"doc_value\"] = doc_cents / 100.0\n",
    "    \n",
    "    # lines per doc: use target_qty_per_line to reduce lines, then clamp\n",
    "    unit = pd.to_numeric(plan[\"unit_price\"], errors=\"coerce\").replace([np.inf,-np.inf], np.nan).fillna(1.0).clip(lower=1.0).to_numpy()\n",
    "    avg_lines_cap = plan[\"avg_lines_per_doc\"].to_numpy().astype(int)\n",
    "    dv = plan[\"doc_value\"].to_numpy()\n",
    "\n",
    "    # variable target qty per doc (lognormal multiplier)\n",
    "    # ln(Z) ~ N(0, qty_sigma^2) → E[Z]=exp(0.5*qty_sigma^2)\n",
    "    z = rng.lognormal(mean=0.0, sigma=qty_sigma, size=len(plan))\n",
    "    tq = np.maximum(1.0, target_qty_per_line * z)\n",
    "\n",
    "    # expected lines for each doc\n",
    "    lam = np.maximum(1.0, dv / (unit * tq))\n",
    "    # sample Poisson around that expectation (min 1, then cap)\n",
    "    n_lines = np.clip(rng.poisson(lam=lam).astype(int), 1, avg_lines_cap)\n",
    "    plan[\"n_lines\"] = n_lines\n",
    "\n",
    "    # dates\n",
    "    if df_date is None:\n",
    "        df_date = generate_dim_date(2020, 2025)\n",
    "    plan[\"date\"] = sample_dates(df_date, size=len(plan), ensure_quarter_balance=ensure_quarter_balance, rng=rng)\n",
    "\n",
    "    keep = [\n",
    "        \"item_name\", \"source_type\", \"date\", \"doc_value\", \"n_lines\", \"unit_price\",\n",
    "        \"product_id\", \"procurement_id\", \"service_id\"\n",
    "    ]\n",
    "    keep = [c for c in keep if c in plan.columns]\n",
    "    \n",
    "    return plan[keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6) Fast: mapping index + materialize lines (vectorized-ish)\n",
    "# =============================================================================\n",
    "\n",
    "def build_mapping_index(\n",
    "    df_mapping: pd.DataFrame,\n",
    "    *,\n",
    "    schema: Schema = S,\n",
    "    passthrough: tuple[str, ...] = (\"vendor_name\", \"bu_id\", \"customer_name\"),  # add more if you like\n",
    ") -> dict:\n",
    "    dfm = df_mapping.copy()\n",
    "    # Safety: coalesce if duplicates slipped through\n",
    "    dfm = coalesce_duplicate_named_columns(dfm, schema.account_id)\n",
    "\n",
    "    if \"item_name\" not in dfm.columns:\n",
    "        raise ValueError(\"df_mapping must contain 'item_name'.\")\n",
    "    if schema.account_id not in dfm.columns:\n",
    "        raise ValueError(f\"df_mapping must contain canonical '{schema.account_id}'.\")\n",
    "\n",
    "    # keep only columns we need for the index\n",
    "    keep_cols = [\"item_name\", schema.account_id] + [c for c in passthrough if c in dfm.columns]\n",
    "    dfm = dfm[keep_cols]\n",
    "\n",
    "    index: dict = {}\n",
    "    for name, g in dfm.groupby(\"item_name\", sort=False):\n",
    "        entry = {schema.account_id: g[schema.account_id].to_numpy()}\n",
    "        for col in passthrough:\n",
    "            if col in g.columns:\n",
    "                entry[col] = g[col].to_numpy()\n",
    "        index[name] = entry\n",
    "    return index\n",
    "\n",
    "\n",
    "def fast_materialize_lines(\n",
    "    plan: pd.DataFrame,\n",
    "    mapping_index: dict,\n",
    "    *,\n",
    "    rng=None,\n",
    "    schema=S,\n",
    "    qty_line_sigma: float = 0.5,   \n",
    "    min_qty: float = 2.0,\n",
    "    max_qty: float | None = None,  # e.g., 50 or None\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(44)\n",
    "\n",
    "    n_lines_per_doc = plan[\"n_lines\"].to_numpy().astype(int)\n",
    "    idx_rep = np.repeat(np.arange(len(plan)), n_lines_per_doc)\n",
    "    lines = plan.iloc[idx_rep].copy().reset_index(drop=True)\n",
    "\n",
    "    # --- split values into lines (keep your multinomial on cents) ---\n",
    "    doc_cents = (plan[\"doc_value\"].to_numpy() * 100).astype(int)\n",
    "    cents_blocks = [\n",
    "        rng.multinomial(c, np.ones(k)/k) if k > 0 else np.array([0], int)\n",
    "        for c, k in zip(doc_cents, n_lines_per_doc)\n",
    "    ]\n",
    "    line_cents = np.concatenate(cents_blocks)\n",
    "    amounts = line_cents / 100.0\n",
    "\n",
    "    # --- sign & D/C ---\n",
    "    st = lines.get(\"source_type\", \"\").astype(str).str.lower().to_numpy()\n",
    "    sign = np.where(np.isin(st, [\"service\",\"procurement\",\"overhead\"]), -1.0, 1.0)\n",
    "    amounts *= sign\n",
    "    lines[schema.amount] = amounts\n",
    "\n",
    "    # --- quantity with line-level randomness ---\n",
    "    unit = pd.to_numeric(lines.get(\"unit_price\", 1.0), errors=\"coerce\").replace([np.inf,-np.inf], np.nan).fillna(1.0).clip(lower=1.0).to_numpy()\n",
    "    # multiplicative noise per line\n",
    "    qnoise = rng.lognormal(mean=0.0, sigma=qty_line_sigma, size=len(lines))\n",
    "    raw_qty = np.abs(amounts) / np.maximum(unit, 1.0) * qnoise\n",
    "    qty = np.ceil(np.clip(raw_qty, min_qty, max_qty if max_qty is not None else np.inf))\n",
    "    lines[schema.quantity] = qty\n",
    "\n",
    "    lines[schema.debit_credit] = np.where(amounts >= 0, \"Credit\", \"Debit\")\n",
    "\n",
    "    # --- mapping assignment (account_id + passthroughs) ---\n",
    "    item_arr = lines[\"item_name\"].to_numpy()\n",
    "    acc = np.empty(len(lines), dtype=object)\n",
    "\n",
    "    # discover which extra fields exist in the index\n",
    "    extra_cols = set()\n",
    "    for entry in mapping_index.values():\n",
    "        extra_cols.update(k for k in entry.keys() if k != schema.account_id)\n",
    "    extra_cols = list(extra_cols)\n",
    "    extra_vals = {c: np.empty(len(lines), dtype=object) for c in extra_cols}\n",
    "\n",
    "    groups = pd.Series(np.arange(len(lines))).groupby(item_arr)\n",
    "    for name, idxs in groups:\n",
    "        idxs = idxs.values\n",
    "        pool = mapping_index.get(name)\n",
    "        if not pool or len(pool[schema.account_id]) == 0:\n",
    "            acc[idxs] = None\n",
    "            for c in extra_cols: extra_vals[c][idxs] = None\n",
    "            continue\n",
    "        k = len(pool[schema.account_id])\n",
    "        sel = rng.integers(0, k, size=len(idxs))\n",
    "        acc[idxs] = pool[schema.account_id][sel]\n",
    "        for c in extra_cols:\n",
    "            extra_vals[c][idxs] = pool.get(c, np.array([None]))[sel] if c in pool else None\n",
    "\n",
    "    lines[schema.account_id] = acc\n",
    "    for c in extra_cols:\n",
    "        lines[c] = extra_vals[c]\n",
    "\n",
    "    keep = [\n",
    "        \"date\", schema.amount, schema.quantity, schema.debit_credit, schema.account_id,\n",
    "        \"item_name\", \"source_type\", \"unit_price\",\n",
    "        # your IDs if present in plan:\n",
    "        schema.product_id, schema.procurement_id, schema.service_id,\n",
    "        # passthroughs:\n",
    "        \"vendor_name\", \"bu_id\", \"customer_name\"\n",
    "    ]\n",
    "    keep = [c for c in keep if c in lines.columns]\n",
    "    return lines[keep]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7) Metadata (doc numbers, currency) + balancing\n",
    "# =============================================================================\n",
    "\n",
    "def assign_document_numbers(\n",
    "    df_lines: pd.DataFrame,\n",
    "    df_document_metadata: Optional[pd.DataFrame],\n",
    "    *,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    "    schema: Schema = S,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Assign document_number and currency in one vectorized pass.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(46)\n",
    "\n",
    "    out = df_lines.copy()\n",
    "\n",
    "    # document number pool\n",
    "    if df_document_metadata is not None and schema.document_number in df_document_metadata.columns:\n",
    "        doc_pool = df_document_metadata[schema.document_number].dropna().astype(str).to_numpy()\n",
    "        if doc_pool.size == 0:\n",
    "            doc_pool = np.array([f\"DOC-{i:06d}\" for i in range(max(1, len(out) // 10))])\n",
    "    else:\n",
    "        doc_pool = np.array([f\"DOC-{i:06d}\" for i in range(max(1, len(out) // 10))])\n",
    "\n",
    "    out[schema.document_number] = rng.choice(doc_pool, size=len(out), replace=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def fast_balance(\n",
    "    df_lines: pd.DataFrame,\n",
    "    df_accounts: Optional[pd.DataFrame],\n",
    "    *,\n",
    "    tolerance: float = 100.0,\n",
    "    rng: Optional[np.random.Generator] = None,\n",
    "    schema: Schema = S,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Optional: one correction line per document to offset imbalance.\n",
    "    If no Asset accounts are available, returns as-is.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(45)\n",
    "    if df_accounts is None or \"account_type\" not in df_accounts.columns:\n",
    "        return df_lines\n",
    "\n",
    "    assets = df_accounts.loc[df_accounts[\"account_type\"].eq(\"Asset\"), schema.account_id].dropna().to_numpy()\n",
    "    if assets.size == 0:\n",
    "        return df_lines\n",
    "\n",
    "    df = df_lines.copy()\n",
    "    doc_sums = df.groupby(schema.document_number, sort=False)[schema.amount].sum().round(2)\n",
    "    need_fix = doc_sums.index[np.abs(doc_sums.values) > tolerance]\n",
    "    if len(need_fix) == 0:\n",
    "        return df\n",
    "\n",
    "    sample_rows = (\n",
    "        df.set_index(schema.document_number)\n",
    "          .loc[need_fix]\n",
    "          .groupby(level=0, sort=False)\n",
    "          .nth(0)\n",
    "          .reset_index()\n",
    "    )\n",
    "    fixes = []\n",
    "    for _, r in sample_rows.iterrows():\n",
    "        imbalance = float(doc_sums.loc[r[schema.document_number]])\n",
    "        signed_amt = -imbalance  # exact offset\n",
    "        fixes.append({\n",
    "            schema.document_number: r[schema.document_number],\n",
    "            schema.debit_credit: \"Debit\" if signed_amt > 0 else \"Credit\",\n",
    "            schema.date: r[schema.date],\n",
    "            schema.amount: signed_amt,\n",
    "            schema.quantity: -1,\n",
    "            schema.account_id: assets[rng.integers(0, len(assets))],\n",
    "            schema.product_id: None, schema.procurement_id: None, schema.service_id: None,\n",
    "            \"item_name\": \"Balance Correction\",\n",
    "            \"source_type\": \"correction\",\n",
    "        })\n",
    "\n",
    "    return pd.concat([df, pd.DataFrame(fixes)], ignore_index=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8) Top-level pipeline\n",
    "# =============================================================================\n",
    "\n",
    "def create_erp_data(\n",
    "    df_expenses: pd.DataFrame,\n",
    "    df_mapping: pd.DataFrame,\n",
    "    df_document_metadata: Optional[pd.DataFrame] = None,\n",
    "    df_accounts: Optional[pd.DataFrame] = None,\n",
    "    *,\n",
    "    schema: Schema = S,\n",
    "    year_start: int = 2020,\n",
    "    year_end: int = 2025,\n",
    "    seed: int = 12345,\n",
    "    ensure_quarter_balance: bool = True,\n",
    "    max_lines_per_doc: int = 10,\n",
    "    cap_factor: float = 1.15,\n",
    "    balance_documents: bool = False,\n",
    "    balance_tolerance: float = 100.0,\n",
    "    target_qty_per_line: float = 4.0,\n",
    "    qty_sigma: float = 0.5,\n",
    "    lines_for_band: tuple[int, int] = (12, 2),\n",
    "    min_doc_value: float = 5_000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    High-speed, generalizable ERP data generator.\n",
    "    Required minimal columns in df_expenses: ['item_name','source_type','annual_spend','unit_price']\n",
    "    Required minimal columns in df_mapping:  ['item_name','account_id'] (+ optional passthrough dims)\n",
    "    df_document_metadata optional: columns 'document_number','currency' improve realism.\n",
    "    df_accounts optional (for balancing): must include ['account_id','account_type'].\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # 1) Normalize once (safe)\n",
    "    df_expenses = normalize_df(df_expenses, schema)\n",
    "    df_mapping  = normalize_df(df_mapping,  schema)\n",
    "    if df_document_metadata is not None:\n",
    "        df_document_metadata = normalize_df(df_document_metadata, schema)\n",
    "    if df_accounts is not None:\n",
    "        df_accounts = normalize_df(df_accounts, schema)\n",
    "\n",
    "    # 1a) Coalesce any duplicate canonical IDs introduced by normalization\n",
    "    for c in [schema.account_id, schema.customer_id, schema.vendor_id]:\n",
    "        if c in df_mapping.columns:\n",
    "            df_mapping = coalesce_duplicate_named_columns(df_mapping, c)\n",
    "\n",
    "    # 2) Validate + light sanitization\n",
    "    need_exp = {\"item_name\", \"source_type\", \"unit_price\"}\n",
    "    if not need_exp.issubset(df_expenses.columns):\n",
    "        missing = need_exp - set(df_expenses.columns)\n",
    "        raise ValueError(f\"df_expenses missing required columns: {sorted(missing)}\")\n",
    "    if not any(c in df_expenses.columns for c in (\"annual_spend\", \"total_amount\", \"total_ammount\", \"total\")):\n",
    "        raise ValueError(\"df_expenses must have a total column like 'annual_spend'\")\n",
    "    if \"item_name\" not in df_mapping.columns or schema.account_id not in df_mapping.columns:\n",
    "        raise ValueError(\"df_mapping must have 'item_name' and canonical 'account_id'\")\n",
    "\n",
    "    # keep unit_price sane to avoid huge quantities / divide-by-zero\n",
    "    df_expenses[\"unit_price\"] = (\n",
    "        pd.to_numeric(df_expenses[\"unit_price\"], errors=\"coerce\")\n",
    "          .replace([np.inf, -np.inf], np.nan)\n",
    "          .fillna(1.0)\n",
    "          .clip(lower=1.0)\n",
    "    )\n",
    "\n",
    "    # 3) Counts per item\n",
    "    df_counts = fast_assign_counts(\n",
    "        df_expenses,\n",
    "        max_lines_per_doc=max_lines_per_doc,\n",
    "        cap_factor=cap_factor,\n",
    "        lines_for_band=lines_for_band,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    # 4) Per-document plan\n",
    "    df_date = generate_dim_date(year_start, year_end, business_days_only=True)\n",
    "    plan = fast_make_doc_plan(\n",
    "        df_counts,\n",
    "        rng=rng,\n",
    "        df_date=df_date,\n",
    "        ensure_quarter_balance=ensure_quarter_balance,\n",
    "        target_qty_per_line=target_qty_per_line, \n",
    "        qty_sigma=qty_sigma,\n",
    "    )\n",
    "\n",
    "    # 5) Mapping index (with passthroughs like vendor_name, bu_id) + materialize\n",
    "    mapping_index = build_mapping_index(df_mapping, schema=schema, passthrough=(\"vendor_name\", \"bu_id\", \"customer_name\"))\n",
    "    lines = fast_materialize_lines(plan, mapping_index, rng=rng, schema=schema)\n",
    "\n",
    "    # 6) Document numbers\n",
    "    lines = assign_document_numbers(lines, df_document_metadata, rng=rng, schema=schema)\n",
    "\n",
    "    # 7) Output (canonical order)\n",
    "    cols = [\n",
    "        schema.document_number, schema.date,\n",
    "        schema.amount, schema.quantity, schema.debit_credit,\n",
    "        schema.account_id, schema.product_id, schema.procurement_id, schema.service_id,\n",
    "        \"item_name\", \"source_type\", \"unit_price\", \"bu_id\", schema.customer_id, schema.vendor_id\n",
    "    ]\n",
    "    cols = [c for c in cols if c in lines.columns]\n",
    "    out = lines[cols].copy()\n",
    "\n",
    "    # 8) Optional balancing\n",
    "    if balance_documents and df_accounts is not None:\n",
    "        out = fast_balance(out, df_accounts, tolerance=balance_tolerance, rng=rng, schema=schema)\n",
    "        for col in cols:\n",
    "            if col not in out.columns:\n",
    "                out[col] = None\n",
    "        out = out[cols]\n",
    "\n",
    "    return out.reset_index(drop=True)\n",
    "\n",
    "def mirror_intercompany_expenses_to_products(\n",
    "    df_products: pd.DataFrame,\n",
    "    df_expenses: pd.DataFrame,\n",
    "    *,\n",
    "    intercompany_sales_account=\"Inter Company Gross Sales\",\n",
    "    intercompany_expense_account=\"Inter Company COS\",\n",
    "    account_id_sales=4007,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove existing IC sales from df_products, then for every\n",
    "    intercompany expense row, add two mirrored product rows (+ and -)\n",
    "    copying *all* columns from df_expenses and just changing account fields.\n",
    "    \"\"\"\n",
    "    # 1) Drop existing IC rows from products\n",
    "    df_products_clean = df_products[df_products[\"account_name\"] != intercompany_sales_account].copy()\n",
    "\n",
    "    # 2) Filter for IC expenses\n",
    "    ic_expenses = df_expenses[df_expenses[\"account_name\"] == intercompany_expense_account].copy()\n",
    "    if ic_expenses.empty:\n",
    "        print(\"⚠ No intercompany expenses found — nothing to mirror.\")\n",
    "        return df_products\n",
    "\n",
    "    mirrored_rows = []\n",
    "    for _, row in ic_expenses.iterrows():\n",
    "        # Convert Series to dict to make a copy we can modify\n",
    "        base = row.to_dict()\n",
    "\n",
    "        # Overwrite fields for intercompany sales\n",
    "        base[\"account_name\"] = intercompany_sales_account\n",
    "        base[\"account_id\"] = account_id_sales\n",
    "        base[\"customer_name\"] = row[\"vendor_name\"]  # vendor becomes customer\n",
    "        base[\"debit_credit\"] = \"credit\"  # sales usually credit\n",
    "        base[\"quantity\"] = row[\"quantity\"]\n",
    "        base[\"amount\"] = abs(row[\"amount\"])  # make it positive\n",
    "\n",
    "        mirrored_rows.append(base)  # positive row\n",
    "\n",
    "        # Create contra row (negative)\n",
    "        contra = base.copy()\n",
    "        contra[\"amount\"] = -contra[\"amount\"]\n",
    "        contra[\"debit_credit\"] = \"Debit\"\n",
    "        mirrored_rows.append(contra)\n",
    "\n",
    "    mirrored_df = pd.DataFrame(mirrored_rows)\n",
    "\n",
    "    # 3) Combine and reset index\n",
    "    out = pd.concat([df_products_clean, mirrored_df], ignore_index=True)\n",
    "    return out\n",
    "\n",
    "def balance_monthly(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    date_col: str = \"date\",\n",
    "    amount_col: str = \"amount\",\n",
    "    source_type_col: str = \"source_type\",\n",
    "    target_types: tuple[str, ...] = (\"product\",),\n",
    "    noise_pct: float = 0.10,              # months should be within ±10% of each other\n",
    "    business_days_only: bool = False,     # choose random calendar day or business day\n",
    "    rng: np.random.Generator | None = None,\n",
    "    max_smoothing_passes: int = 6,        # extra swaps/moves to pull months into band\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reassigns dates ONLY for rows whose source_type is in target_types so that, per source_type,\n",
    "    the *absolute* monthly totals are ~even (within ±noise_pct). Length & amounts are unchanged.\n",
    "\n",
    "    Algorithm (per source_type independently):\n",
    "      1) Create a month window from min..max existing dates for that source_type.\n",
    "      2) Target per month = (sum of abs(amount)) / n_months.\n",
    "      3) Shuffle rows, then greedily place each line into the month with the lowest current abs sum.\n",
    "      4) Optional quick smoothing: move/swap a few lines to pull all months within tolerance.\n",
    "      5) Assign a random day inside the chosen month to each line (no time component).\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(42)\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # ensure datetime\n",
    "    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\")\n",
    "\n",
    "    # helper: random day in a Period month (normalized to midnight, no time component)\n",
    "    def _rand_day(period: pd.Period) -> pd.Timestamp:\n",
    "        start = period.to_timestamp(how=\"start\").normalize()\n",
    "        end   = period.to_timestamp(how=\"end\").normalize()\n",
    "        if business_days_only:\n",
    "            days = pd.bdate_range(start, end)\n",
    "            if len(days) == 0:  # fallback to calendar days if no BDays\n",
    "                days = pd.date_range(start, end, freq=\"D\")\n",
    "        else:\n",
    "            days = pd.date_range(start, end, freq=\"D\")\n",
    "        return pd.Timestamp(rng.choice(days.values)).normalize()\n",
    "\n",
    "    # normalize target types for matching\n",
    "    targets = {str(t).strip() for t in target_types}\n",
    "\n",
    "    for stype in sorted(targets):\n",
    "        # indices for this source_type\n",
    "        sub_idx = out.index[out[source_type_col].astype(str).str.strip() == stype]\n",
    "        if len(sub_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        sub = out.loc[sub_idx, [date_col, amount_col]].copy()\n",
    "        # guard against NaNs\n",
    "        amts = pd.to_numeric(sub[amount_col], errors=\"coerce\").fillna(0.0).to_numpy(dtype=float)\n",
    "\n",
    "        # Need a month window — use existing date range from current data; if all NaT, skip\n",
    "        if sub[date_col].notna().any():\n",
    "            start_month = sub[date_col].min().normalize().replace(day=1)\n",
    "            end_month   = (sub[date_col].max() + pd.offsets.MonthEnd(0)).normalize()\n",
    "        else:\n",
    "            # if no dates at all, we can't infer a window; skip this type\n",
    "            continue\n",
    "\n",
    "        months = pd.period_range(start=start_month, end=end_month, freq=\"M\")\n",
    "        m = len(months)\n",
    "        if m == 0:\n",
    "            continue\n",
    "\n",
    "        # Target per month based on absolute totals\n",
    "        total_abs = float(np.abs(amts).sum())\n",
    "        target = total_abs / m\n",
    "        tol = abs(target) * noise_pct\n",
    "\n",
    "        n = len(sub_idx)\n",
    "        # Randomize traversal order to avoid bias\n",
    "        order = rng.permutation(n)\n",
    "\n",
    "        # Greedy fill: always place the next row into the lightest (lowest abs sum) month\n",
    "        month_abs_sums = np.zeros(m, dtype=float)\n",
    "        assignment = np.empty(n, dtype=np.int32)\n",
    "\n",
    "        for pos in order:\n",
    "            # choose month whose abs total is lowest\n",
    "            k = int(np.argmin(month_abs_sums))\n",
    "            assignment[pos] = k\n",
    "            month_abs_sums[k] += abs(amts[pos])\n",
    "\n",
    "        # Quick smoothing: attempt to move items from high to low until within tolerance or limit reached\n",
    "        def _within_tolerance(sums: np.ndarray) -> bool:\n",
    "            return (sums.max() - sums.min()) <= 2 * tol or np.all(\n",
    "                (np.abs(sums - target) <= tol)  # stricter per-month check\n",
    "            )\n",
    "\n",
    "        passes = 0\n",
    "        while passes < max_smoothing_passes and not _within_tolerance(month_abs_sums):\n",
    "            hi = int(np.argmax(month_abs_sums))\n",
    "            lo = int(np.argmin(month_abs_sums))\n",
    "            if hi == lo:\n",
    "                break\n",
    "\n",
    "            # candidates in high month\n",
    "            cand_idx = np.where(assignment == hi)[0]\n",
    "            if cand_idx.size == 0:\n",
    "                break\n",
    "\n",
    "            # move the candidate that best reduces (max deviation + min deviation)\n",
    "            c_abs = np.abs(amts[cand_idx])\n",
    "            # effect on sums if moved\n",
    "            after_hi = month_abs_sums[hi] - c_abs\n",
    "            after_lo = month_abs_sums[lo] + c_abs\n",
    "\n",
    "            # score: sum of deviations from target after move (lower is better)\n",
    "            score = np.abs(after_hi - target) + np.abs(after_lo - target)\n",
    "            best = cand_idx[int(np.argmin(score))]\n",
    "\n",
    "            # perform move\n",
    "            month_abs_sums[hi] -= abs(amts[best])\n",
    "            month_abs_sums[lo] += abs(amts[best])\n",
    "            assignment[best] = lo\n",
    "\n",
    "            passes += 1\n",
    "\n",
    "        # Assign actual calendar dates inside each month (no time component)\n",
    "        new_dates = np.empty(n, dtype=\"datetime64[ns]\")\n",
    "        for k in range(m):\n",
    "            rows_k = np.where(assignment == k)[0]\n",
    "            if rows_k.size == 0:\n",
    "                continue\n",
    "            month_k = months[k]\n",
    "            # independent random day for each row\n",
    "            new_dates[rows_k] = np.array([_rand_day(month_k).to_datetime64() for _ in range(rows_k.size)])\n",
    "\n",
    "        # write back dates for this source_type\n",
    "        out.loc[sub_idx, date_col] = pd.to_datetime(new_dates)\n",
    "\n",
    "    return out\n",
    "\n",
    "def estimate_costs_from_payroll(\n",
    "    df_pay: pd.DataFrame,\n",
    "    product_multiplier: float = 3,\n",
    "    service_multiplier: float = 1.5,\n",
    "    overhead_multiplier: float = 0.2,\n",
    "    revenue_multiplier: float = 7,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Estimate total product, service, and overhead costs based on payroll data.\n",
    "    \"\"\"\n",
    "    temp_df = df_pay[df_pay[\"line_id\"] == \"Monthly-pay\"]\n",
    "    total_payroll = temp_df[\"amount\"].sum()\n",
    "    total_payroll = total_payroll/10\n",
    "\n",
    "    return {\n",
    "        \"estimated_payroll\": total_payroll,\n",
    "        \"estimated_product\": round(total_payroll * product_multiplier),\n",
    "        \"estimated_service\": round(total_payroll * service_multiplier),\n",
    "        \"estimated_overhead\": round(total_payroll * overhead_multiplier),\n",
    "        \"estimated_revenue\": round(total_payroll * revenue_multiplier)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_all_erp_data(generated_mapped_data: dict, company_name: str, save_to_csv: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Creates all ERP-related data by combining generated and mapped data.\n",
    "    \"\"\"\n",
    "\n",
    "    df_map_products = generated_mapped_data[\"df_map_products\"]\n",
    "    df_map_expenses = generated_mapped_data[\"df_map_expenses\"]\n",
    "    df_erp_products = generated_mapped_data[\"df_erp_products\"]\n",
    "    df_erp_expenses = generated_mapped_data[\"df_erp_expenses\"]\n",
    "\n",
    "    document_metadata_expense = random_generators.generate_document_metadata(n=30, start_index=1000)\n",
    "    document_metadata_products = random_generators.generate_document_metadata(n=30, start_index=2000)\n",
    "    \n",
    "    df_erp_expenses_full = erp.create_erp_data(df_expenses=df_erp_expenses, df_expenses_mapping=df_map_expenses, df_document_metadata=document_metadata_expense)\n",
    "    df_erp_products_full = erp.create_erp_data(df_expenses=df_erp_products, df_expenses_mapping=df_map_products, df_document_metadata=document_metadata_products)\n",
    "      \n",
    "    # Full target schema # also currency, amount_eur, Type\n",
    "    full_columns = ['document_number', 'type', 'date', 'amount_dkk', 'account_name', 'product_id', 'procurement_id', 'service_id']\n",
    "    vendor_col = ['vendor_name']    \n",
    "    customer_col = ['customer_name']\n",
    "\n",
    "    # Reindex all ERP dataframes to align to full schema\n",
    "    df_expenses_full = df_erp_expenses_full.reindex(columns=full_columns + vendor_col)\n",
    "    df_products_full = df_erp_products_full.reindex(columns=full_columns + customer_col)\n",
    "\n",
    "\n",
    "    rename_cols = {\n",
    "        'document_number': 'document_number',\n",
    "        'type': 'debit_credit', \n",
    "        'date': 'date',\n",
    "        'amount_dkk': 'amount',\n",
    "        'account_name': 'account_id',\n",
    "        'product_id': 'product_id',\n",
    "        'procurement_id': 'procurement_id',\n",
    "        'service_id': 'service_id',\n",
    "        'vendor_name': 'vendor_id',\n",
    "        'customer_name': 'customer_id'}\n",
    "    \n",
    "\n",
    "\n",
    "    df_expenses_full.rename(columns=rename_cols, inplace=True)\n",
    "    df_products_full.rename(columns=rename_cols, inplace=True)\n",
    "    \n",
    "    if save_to_csv:\n",
    "        output_dir = f\"data/outputdata/fact\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_erp_expenses_full.to_csv(f\"{output_dir}/erp_expenses.csv\", index=False)\n",
    "        df_erp_products_full.to_csv(f\"{output_dir}/erp_products.csv\", index=False)\n",
    "\n",
    "\n",
    "    # Concatenate all ERP data\n",
    "    df_erp_all = pd.concat([df_expenses_full, df_products_full], ignore_index=True)\n",
    "    df_accounts = pd.read_csv(\"data/outputdata/dimensions/account.csv\")\n",
    "\n",
    "    df_erp_all = erp.balance_documents_with_assets(df_erp=df_erp_all, df_accounts=df_accounts, tolerance=100)\n",
    "    \n",
    "    print(f\"✔ All erp-data generated.\")\n",
    "\n",
    "    if save_to_csv:\n",
    "        output_dir = f\"data/outputdata/fact\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        df_erp_expenses_full.to_csv(f\"{output_dir}/erp_expenses.csv\", index=False)\n",
    "        df_erp_products_full.to_csv(f\"{output_dir}/erp_products.csv\", index=False)\n",
    "        df_erp_all.to_csv(f\"{output_dir}/general_ledger.csv\", index=False)\n",
    "        print(f\"✔ All ERP CSVs saved to: {output_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"df_erp_expenses_full\": df_erp_expenses_full,\n",
    "        \"df_erp_products_full\": df_erp_products_full,\n",
    "        \"df_erp_all\": df_erp_all,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45abb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_erp_data_from_driver_rows(\n",
    "    df_driver: pd.DataFrame,\n",
    "    *,\n",
    "    df_document_metadata: Optional[pd.DataFrame] = None,\n",
    "    df_accounts: Optional[pd.DataFrame] = None,\n",
    "    category_sign_map: Optional[Dict[str, int]] = None,\n",
    "    seed: int = 4321,\n",
    "    ensure_quarter_balance: bool = True,\n",
    "    max_lines_per_doc: int = 15,\n",
    "    cap_factor: float = 1.2,\n",
    "    target_qty_per_line: float = 6.0,\n",
    "    qty_sigma: float = 0.55,\n",
    "    lines_for_band: tuple[int, int] = (10, 3),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Turn aggregated driver rows into synthetic ERP line items.\"\"\"\n",
    "\n",
    "    required_cols = {\n",
    "        \"company\",\n",
    "        \"AccountKey\",\n",
    "        \"account_name\",\n",
    "        \"item_name\",\n",
    "        \"category\",\n",
    "        \"annual_spend\",\n",
    "        \"unit_price\",\n",
    "        \"party_id\",\n",
    "        \"party_name\",\n",
    "        \"bu_id\",\n",
    "        \"bu_name\",\n",
    "    }\n",
    "    missing = required_cols - set(df_driver.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"df_driver is missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    df_driver = df_driver.copy()\n",
    "    df_driver[\"annual_spend\"] = (\n",
    "        pd.to_numeric(df_driver[\"annual_spend\"], errors=\"coerce\")\n",
    "        .fillna(0.0)\n",
    "        .clip(lower=0.0)\n",
    "    )\n",
    "    df_driver[\"unit_price\"] = (\n",
    "        pd.to_numeric(df_driver[\"unit_price\"], errors=\"coerce\")\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .fillna(1.0)\n",
    "        .clip(lower=1.0)\n",
    "    )\n",
    "\n",
    "    df_driver[\"source_type\"] = df_driver[\"category\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "    revenue_mask = df_driver[\"source_type\"].eq(\"revenue\")\n",
    "    expense_mask = df_driver[\"source_type\"].isin({\"cogs\", \"fixedcost\", \"ebit\"})\n",
    "\n",
    "    df_driver[\"customer_name\"] = np.where(revenue_mask, df_driver[\"party_name\"], None)\n",
    "    df_driver[\"vendor_name\"] = np.where(expense_mask, df_driver[\"party_name\"], None)\n",
    "\n",
    "    df_expenses = df_driver[\n",
    "        [\n",
    "            \"item_name\",\n",
    "            \"source_type\",\n",
    "            \"annual_spend\",\n",
    "            \"unit_price\",\n",
    "            \"account_name\",\n",
    "            \"customer_name\",\n",
    "            \"vendor_name\",\n",
    "            \"bu_id\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    df_mapping = df_driver[\n",
    "        [\n",
    "            \"item_name\",\n",
    "            \"account_name\",\n",
    "            \"customer_name\",\n",
    "            \"vendor_name\",\n",
    "            \"bu_id\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    erp_df = create_erp_data(\n",
    "        df_expenses=df_expenses,\n",
    "        df_mapping=df_mapping,\n",
    "        df_document_metadata=df_document_metadata,\n",
    "        df_accounts=df_accounts,\n",
    "        seed=seed,\n",
    "        ensure_quarter_balance=ensure_quarter_balance,\n",
    "        max_lines_per_doc=max_lines_per_doc,\n",
    "        cap_factor=cap_factor,\n",
    "        target_qty_per_line=target_qty_per_line,\n",
    "        qty_sigma=qty_sigma,\n",
    "        lines_for_band=lines_for_band,\n",
    "    )\n",
    "\n",
    "    meta = df_driver[\n",
    "        [\n",
    "            \"item_name\",\n",
    "            \"source_type\",\n",
    "            \"account_name\",\n",
    "            \"customer_name\",\n",
    "            \"vendor_name\",\n",
    "            \"AccountKey\",\n",
    "            \"company\",\n",
    "            \"bu_id\",\n",
    "            \"bu_name\",\n",
    "            \"party_id\",\n",
    "            \"party_name\",\n",
    "            \"category\",\n",
    "        ]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    meta_key_cols = [\n",
    "        \"item_name\",\n",
    "        \"source_type\",\n",
    "        \"account_name\",\n",
    "        \"customer_name\",\n",
    "        \"vendor_name\",\n",
    "        \"bu_id\",\n",
    "    ]\n",
    "\n",
    "    meta_merge = meta.copy()\n",
    "    erp_merge = erp_df.copy()\n",
    "    for col in (\"customer_name\", \"vendor_name\"):\n",
    "        if col not in erp_merge:\n",
    "            erp_merge[col] = np.nan\n",
    "        meta_merge[col] = meta_merge[col].fillna(\"\")\n",
    "        erp_merge[col] = erp_merge[col].fillna(\"\")\n",
    "\n",
    "    meta_merge[\"bu_id\"] = meta_merge[\"bu_id\"].astype(str)\n",
    "    if \"bu_id\" in erp_merge:\n",
    "        erp_merge[\"bu_id\"] = erp_merge[\"bu_id\"].astype(str)\n",
    "\n",
    "    enriched = erp_merge.merge(\n",
    "        meta_merge,\n",
    "        how=\"left\",\n",
    "        on=meta_key_cols,\n",
    "        suffixes=(\"\", \"_meta\"),\n",
    "    )\n",
    "\n",
    "    category_sign_map = category_sign_map or {\n",
    "        \"revenue\": -1,\n",
    "        \"cogs\": 1,\n",
    "        \"fixedcost\": 1,\n",
    "        \"ebit\": 1,\n",
    "        \"balancesheet\": 1,\n",
    "    }\n",
    "\n",
    "    enriched[\"source_type\"] = enriched[\"source_type\"].astype(str).str.lower()\n",
    "    signs = enriched[\"source_type\"].map(category_sign_map).fillna(1)\n",
    "    enriched[\"amount\"] = enriched[\"amount\"].abs() * signs\n",
    "    enriched[\"debit_credit\"] = np.where(enriched[\"amount\"] >= 0, \"Debit\", \"Credit\")\n",
    "\n",
    "    enriched.rename(columns={\"date\": \"posting_date\"}, inplace=True)\n",
    "\n",
    "    for col in (\"customer_name\", \"vendor_name\"):\n",
    "        enriched[col] = enriched[col].replace(\"\", np.nan)\n",
    "\n",
    "    ordered_cols = [\n",
    "        \"document_number\",\n",
    "        \"posting_date\",\n",
    "        \"company\",\n",
    "        \"bu_id\",\n",
    "        \"bu_name\",\n",
    "        \"party_id\",\n",
    "        \"party_name\",\n",
    "        \"AccountKey\",\n",
    "        \"account_name\",\n",
    "        \"item_name\",\n",
    "        \"category\",\n",
    "        \"debit_credit\",\n",
    "        \"amount\",\n",
    "        \"quantity\",\n",
    "        \"unit_price\",\n",
    "        \"customer_name\",\n",
    "        \"vendor_name\",\n",
    "    ]\n",
    "    ordered_cols = [c for c in ordered_cols if c in enriched.columns]\n",
    "    remaining_cols = [c for c in enriched.columns if c not in ordered_cols]\n",
    "    return enriched[ordered_cols + remaining_cols].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_document_metadata: Optional[pd.DataFrame] = None,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DemoDataVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
